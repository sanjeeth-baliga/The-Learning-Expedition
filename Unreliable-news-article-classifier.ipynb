{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8317,"databundleVersionId":109592,"sourceType":"competition"}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sanprofnext/roberta-fake-news-classifier?scriptVersionId=156275956\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Fake News Detection with Roberta LLM","metadata":{}},{"cell_type":"markdown","source":"The algorithm laid out below was developed as a solution to the challenge titled [Fake News](https://www.kaggle.com/competitions/fake-news) in kaggle. This algorithm subjects a Roberta Base LLM to transfer learning and tailors it for the purpose of identifying unreliable news articles\n\nThis technique generated a private and public score of 1.0 on the test data provided within the Kaggle competition","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-24T04:34:00.099219Z","iopub.execute_input":"2023-12-24T04:34:00.099468Z","iopub.status.idle":"2023-12-24T04:34:00.446899Z","shell.execute_reply.started":"2023-12-24T04:34:00.099445Z","shell.execute_reply":"2023-12-24T04:34:00.445956Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/fake-news/submit.csv\n/kaggle/input/fake-news/train.csv\n/kaggle/input/fake-news/test.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 1. Module Installation and Library Import","metadata":{}},{"cell_type":"markdown","source":"Install the required dependencies and import the essential libraries for building the algorithm. Also activate the GPU environment for ensuring parallelization of operations and supercharging the transformer computations","metadata":{}},{"cell_type":"code","source":"!pip install transformers\n!pip install torchmetrics","metadata":{"execution":{"iopub.status.busy":"2023-12-23T09:48:40.269426Z","iopub.execute_input":"2023-12-23T09:48:40.269959Z","iopub.status.idle":"2023-12-23T09:49:06.152906Z","shell.execute_reply.started":"2023-12-23T09:48:40.269926Z","shell.execute_reply":"2023-12-23T09:49:06.151988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install Dataset","metadata":{"execution":{"iopub.status.busy":"2023-12-23T09:49:06.15419Z","iopub.execute_input":"2023-12-23T09:49:06.15452Z","iopub.status.idle":"2023-12-23T09:49:21.545376Z","shell.execute_reply.started":"2023-12-23T09:49:06.154493Z","shell.execute_reply":"2023-12-23T09:49:21.54447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, BertModel, RobertaForSequenceClassification\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torchmetrics import Accuracy\nfrom datasets import Dataset\nimport datasets\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2023-12-23T09:49:21.548573Z","iopub.execute_input":"2023-12-23T09:49:21.549004Z","iopub.status.idle":"2023-12-23T09:49:33.072221Z","shell.execute_reply.started":"2023-12-23T09:49:21.548965Z","shell.execute_reply":"2023-12-23T09:49:33.071413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Data intake, cleanup and tokenization","metadata":{}},{"cell_type":"markdown","source":"Clean-up the training data by imputing any missing values with suitable placeholders. Concatenate every article body with the title and author to prevent loss of relevant information during the training process","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/fake-news/train.csv')\n\n#Handle any missing values by substituting them with placeholder text\ndata['author'] = data['author'].fillna('Unknown',axis=0)\ndata['title'] = data['title'].fillna('Unknown',axis=0)\ndata['text'] = data['text'].fillna('Not Available',axis=0)\n\n#Concatenate the article body with title and author fields to prevent loss of valuable textual training samples\ndata['comb_news'] = 'Title:'+data['title']+'\\nAuthor:'+data['author']+'\\nBody:'+data['text']\n\n#Extract only the concatenated and the corresponding label indicating the authenticity of news from input data\ndata_in = data[['comb_news','label']]","metadata":{"execution":{"iopub.status.busy":"2023-12-23T09:49:33.073276Z","iopub.execute_input":"2023-12-23T09:49:33.073706Z","iopub.status.idle":"2023-12-23T09:49:36.049325Z","shell.execute_reply.started":"2023-12-23T09:49:33.07368Z","shell.execute_reply":"2023-12-23T09:49:36.048542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Instantiate a tokenizer tied to the Roberta model and define a function for applying the tokenizer on each training sample. The function is designed to take a Dataset structure as input and tokenize the structure into fixed chunks of 128 tokens each. Each tokenized chunk is subsequently encoded for inserting into the model   ","metadata":{}},{"cell_type":"code","source":"#Instantiate the tokenizer linked to the Roberta model\nmodel_name = 'roberta-base'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n#Define the function for tokenizing the input data\ndef tokenize(el):\n    #Split every tokenized article into chunks of 128 tokens to ensure streamlined processing. If the size of\n    #the last sample chunk is less than 128, it can be padded for attaining a token length of 128\n    result =  tokenizer(el['comb_news'],truncation=True,max_length=128,padding='max_length',return_overflowing_tokens=True)\n    \n    #Ensure that the remaining fields are correctly mapped to each chunk associated with the original sample\n    sample_map = result.pop('overflow_to_sample_mapping')\n    for key,value in el.items():\n        result[key] = [value[i] for i in sample_map]\n        \n    return result","metadata":{"execution":{"iopub.status.busy":"2023-12-23T09:49:36.050597Z","iopub.execute_input":"2023-12-23T09:49:36.051382Z","iopub.status.idle":"2023-12-23T09:49:37.346156Z","shell.execute_reply.started":"2023-12-23T09:49:36.051342Z","shell.execute_reply":"2023-12-23T09:49:37.345438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split the input data frame into training and validation data frames for training the Roberta LLM through transfer learning. Translate the data frames into Dataset structures for the purpose of tokenization. The tokenized datasets are translated into tensors for collation into dataloaders that are passed as batches into the model","metadata":{}},{"cell_type":"code","source":"#Function used to tokenize the data and package the tokenized into data into data loaders for model ingestion\ndef create_loaders(data_in):\n    #Split the input data columns into training and validation datasets \n    train_data, test_data, train_label, test_label = train_test_split(data_in['comb_news'],data_in['label'],test_size=0.25,random_state=42)\n    \n    #Translate the training and validation data frames into Dataset structures for data pre-processing stage\n    data_train = Dataset.from_pandas(pd.concat([train_data,train_label],axis=1))\n    data_valid = Dataset.from_pandas(pd.concat([test_data,test_label],axis=1))\n    \n    #Create a combined dataset dictionary from the Dataset structures and subject it to the tokenization operation\n    tr_data = datasets.DatasetDict({'train':data_train,'valid':data_valid})\n    tok_data = tr_data.map(tokenize,batched=True)\n    \n    #Remove unwanted columns once the encoded data compatible with the LLM is generated\n    tok_data = tok_data.remove_columns(['comb_news','__index_level_0__'])\n    \n    #Re-package the encoded data, attention masks and labels into tensors for insertion into the model \n    tok_data.set_format('pandas')\n    train_in = tok_data['train'][:]\n    train_set = TensorDataset(torch.tensor(train_in['input_ids']),torch.tensor(train_in['attention_mask']),torch.tensor(train_in['label']))\n    valid_in = tok_data['valid'][:]\n    valid_set = TensorDataset(torch.tensor(valid_in['input_ids']),torch.tensor(valid_in['attention_mask']),torch.tensor(valid_in['label']))\n    \n    #Collate the tensor datasets into training and validation data loaders for generating batches that can be used for \n    #training the pre-trained Roberta LLM through stochastic gradient descent optimization\n    train_loader = DataLoader(train_set,batch_size=64,shuffle=True)\n    valid_loader = DataLoader(valid_set,batch_size=64,shuffle=False)\n    \n    return train_loader,valid_loader","metadata":{"execution":{"iopub.status.busy":"2023-12-23T09:49:37.347191Z","iopub.execute_input":"2023-12-23T09:49:37.347454Z","iopub.status.idle":"2023-12-23T09:49:37.357052Z","shell.execute_reply.started":"2023-12-23T09:49:37.347432Z","shell.execute_reply":"2023-12-23T09:49:37.356216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Store the generated training and validation data loaders\ntrain_loader,valid_loader = create_loaders(data_in)","metadata":{"execution":{"iopub.status.busy":"2023-12-23T09:49:37.35833Z","iopub.execute_input":"2023-12-23T09:49:37.358651Z","iopub.status.idle":"2023-12-23T09:50:54.394721Z","shell.execute_reply.started":"2023-12-23T09:49:37.358621Z","shell.execute_reply":"2023-12-23T09:50:54.393689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Model instantiation and training environment configuration","metadata":{}},{"cell_type":"markdown","source":"Ensure that the Roberta model instance is loaded onto the GPU for accelerated computation. Also unfreeze the feed-forward layers and the last few attention layers for orchestrating the transfer learning procedure","metadata":{}},{"cell_type":"code","source":"#Instantiate the text sequence classification model from HuggingFace model hub based on the Roberta architecture\nmodel = RobertaForSequenceClassification.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2023-12-23T09:50:54.395823Z","iopub.execute_input":"2023-12-23T09:50:54.396111Z","iopub.status.idle":"2023-12-23T09:50:57.555903Z","shell.execute_reply.started":"2023-12-23T09:50:54.396086Z","shell.execute_reply":"2023-12-23T09:50:57.555164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Load the instantiated model onto the GPU for accelerated computation\nmodel = model.to(device)\n\n#Unfreeze the parameters across layers 8, 9 and 10 along with the feed-forward classifier layers for configuring the architecture to perform transfer learning and identify fake news\nfor idx,(name,params) in enumerate(model.named_parameters()):\n    if 'classifier' in name or 'encoder.layer.8' in name or 'encoder.layer.9' in name or 'encoder.layer.10' in name:\n        params.requires_grad = True\n    else:\n        params.requires_grad = False\n\n#Keep a stock of the total number of parameters that would be subjected to adam optimization\ntotal_params = 0\nfor param in model.parameters():\n    if param.requires_grad:\n        total_params+= param.numel()\nprint(total_params)","metadata":{"execution":{"iopub.status.busy":"2023-12-23T09:50:57.5593Z","iopub.execute_input":"2023-12-23T09:50:57.559704Z","iopub.status.idle":"2023-12-23T09:51:02.434041Z","shell.execute_reply.started":"2023-12-23T09:50:57.559666Z","shell.execute_reply":"2023-12-23T09:51:02.433069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Configure the parameters of the training environment in terms of the choice of optimizer, learning rate, loss function for gradient computation and number of epochs","metadata":{}},{"cell_type":"code","source":"epochs=2\noptimizer = torch.optim.AdamW(model.parameters(),lr=5e-5,eps=1e-8)\ncriterion = torch.nn.CrossEntropyLoss()\ntrain_acc,valid_acc = Accuracy(task='binary',num_classes=2).to(device),Accuracy(task='binary',num_classes=2).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-12-23T09:51:02.435034Z","iopub.execute_input":"2023-12-23T09:51:02.435307Z","iopub.status.idle":"2023-12-23T09:51:02.445072Z","shell.execute_reply.started":"2023-12-23T09:51:02.435284Z","shell.execute_reply":"2023-12-23T09:51:02.444254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Model training for Transfer learning to align with the Task context","metadata":{}},{"cell_type":"markdown","source":"Unpack the training batches from data loaders and train configured model layers through adam optimization with the use of cross-entropy as the loss function. Assess the training and validation data accuracy at the end of each epoch. Gradient clipping is also used to avoid the issue of exploding gradients. At the end of every epoch, predict the labels of the validation data batches to assess the performance of the model ","metadata":{}},{"cell_type":"code","source":"for epoch in range(epochs):\n    train_loss, valid_loss = list(),list()\n    print(f'Epoch:{epoch}----------------------->')\n    \n    model.train()\n    for idx,(x_ids,x_mask,x_label) in tqdm(enumerate(train_loader),total=len(train_loader)):\n        optimizer.zero_grad()\n        #Make sure that the upacked training data batches are loaded onto the GPU\n        x_ids, x_mask, x_label = x_ids.to(device), x_mask.to(device), x_label.to(device)\n        #Determine the label predictions from the model and update the parameter with gradients computed on the cross-entropy loss function\n        preds = model(x_ids,attention_mask = x_mask)\n        loss = criterion(preds.logits,x_label)\n        train_loss.append(loss.item())\n        #Determine the training data accuracy at the end of every epoch\n        train_acc.update(torch.argmax(preds.logits,dim=1),x_label)\n        \n        #Clip the computed gradients to prevent the performance constraints posed by exploding gradients\n        torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n        #Update the unfrozen model parameters with each optimizer step\n        loss.backward()\n        optimizer.step()\n        \n    model.eval()\n    for idx,(v_ids,v_mask,v_label) in tqdm(enumerate(valid_loader),total=len(valid_loader)):\n        #Make sure that the upacked validation data batches are loaded onto the GPU\n        v_ids, v_mask, v_label = v_ids.to(device), v_mask.to(device), v_label.to(device)\n        #Determine label predictions on the validation batch\n        preds = model(v_ids,attention_mask = v_mask)\n        #Compute the validation cross-entropy loss for each batch in the validation data\n        loss = criterion(preds.logits,v_label)\n        valid_loss.append(loss.item())\n        valid_acc.update(torch.argmax(preds.logits,dim=1),v_label)\n        \n    #Publish the training and validation cross-entropy losses as well as classificatio accuracy at the end of every iteration\n    avg_train_loss, avg_valid_loss = sum(train_loss)/len(train_loss),sum(valid_loss)/len(valid_loss)\n    print(f'Training loss:{avg_train_loss}\\tValidation loss:{avg_valid_loss}')\n    print(f'Training accuracy:{train_acc.compute().item()}\\tValidation accuracy:{valid_acc.compute().item()}')","metadata":{"execution":{"iopub.status.busy":"2023-12-23T09:51:02.446094Z","iopub.execute_input":"2023-12-23T09:51:02.446371Z","iopub.status.idle":"2023-12-23T11:17:58.58713Z","shell.execute_reply.started":"2023-12-23T09:51:02.446347Z","shell.execute_reply":"2023-12-23T11:17:58.586231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Determine the model performance on the challenge test data for submission","metadata":{}},{"cell_type":"markdown","source":"Define two helper functions for post-processing the test predictions before submission","metadata":{}},{"cell_type":"code","source":"def flatten_data(data):\n    flat_list = list()\n    for item in data:\n        flat_list += item.tolist()\n    return flat_list\n\ndef find_issue(d_out):\n    check_issue = pd.crosstab(d_out['id'],d_out['label']).reset_index().rename(columns={'id':'index',0:'label_0',1:'label_1'})\n    check_issue['issue_flag'] = check_issue.apply(lambda x: x['label_0']>0 & x['label_1']>0,axis=1)\n    errors = check_issue.shape[0]-check_issue['issue_flag'].value_counts().to_frame().reset_index().loc[0,'count']\n    return errors","metadata":{"execution":{"iopub.status.busy":"2023-12-23T12:49:58.473025Z","iopub.execute_input":"2023-12-23T12:49:58.474103Z","iopub.status.idle":"2023-12-23T12:49:58.480547Z","shell.execute_reply.started":"2023-12-23T12:49:58.474063Z","shell.execute_reply":"2023-12-23T12:49:58.479718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Extract the test data for submission creation\nd_test = pd.read_csv('/kaggle/input/fake-news/test.csv')\n\n#Define a function for determination of test predictions and its post-processing into the submission template\ndef generate_result(test):\n    #Impute any missing values with suitable placeholders to prevent loss of valuable information\n    test['author'] = test['author'].fillna('Unknown',axis=0)\n    test['title'] = test['title'].fillna('Unknown',axis=0)\n    test['text'] = test['text'].fillna('Not Available',axis=0)\n    \n    #Concatenate the article body with title and author fields to mirror the pre-processing performed on training data\n    test['comb_news'] = 'Title:'+test['title']+'\\nAuthor:'+test['author']+'\\nBody:'+test['text']\n    d_test_in = d_test[['id','comb_news']]\n    \n    #Translate the data into Dataset structure to mirror the pre-processing performed on training data\n    test_dset = Dataset.from_pandas(d_test_in)\n    \n    #Tokenize the test data with the Roberta tokenizer and remove unwanted columns\n    test_tokens = test_dset.map(tokenize,batched=True)\n    test_tokens = test_tokens.remove_columns(['comb_news'])\n    test_tokens.set_format('pandas')\n    \n    #Package the tokenized data into data loaders for generating model predictions\n    torch_t_data = TensorDataset(torch.tensor(test_tokens['input_ids']),torch.tensor(test_tokens['attention_mask']))\n    test_dataloader = DataLoader(torch_t_data,batch_size=64,shuffle=False)\n    label_preds = list()\n    \n    #Pass every batch from the test data loader into the model for determining the corresponding label predictions\n    model.eval()\n    for idx,(t_inputs,t_amask) in tqdm(enumerate(test_dataloader),total=len(test_dataloader)):\n        t_inputs,t_amask = t_inputs.to(device),t_amask.to(device)\n        preds = model(t_inputs,attention_mask = t_amask)\n        p_label = torch.argmax(preds.logits,dim=1)\n        label_preds.append(p_label)\n    \n    #Post-process the predicted labels with helper functions restructure into a format suitable for submision creation  \n    test_predictions = flatten_data(label_preds)\n    output = pd.concat([test_tokens['id'],pd.DataFrame(test_predictions,columns=['label'])],axis='columns')\n    \n    #Also ensure that the prediction is identical on every token chunk coming from the same news article\n    if find_issue(output)==0:\n        return output.drop_duplicates(subset=['id'])\n    else:\n        return -1","metadata":{"execution":{"iopub.status.busy":"2023-12-23T12:53:57.039289Z","iopub.execute_input":"2023-12-23T12:53:57.040061Z","iopub.status.idle":"2023-12-23T12:53:57.350042Z","shell.execute_reply.started":"2023-12-23T12:53:57.040028Z","shell.execute_reply":"2023-12-23T12:53:57.349071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Generate the submission outputs and create a submission file\nf_output = generate_result(d_test)","metadata":{"execution":{"iopub.status.busy":"2023-12-23T12:54:02.936715Z","iopub.execute_input":"2023-12-23T12:54:02.937346Z","iopub.status.idle":"2023-12-23T12:59:33.724269Z","shell.execute_reply.started":"2023-12-23T12:54:02.937315Z","shell.execute_reply":"2023-12-23T12:59:33.723468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f_output.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-23T13:01:58.103902Z","iopub.execute_input":"2023-12-23T13:01:58.104258Z","iopub.status.idle":"2023-12-23T13:01:58.120828Z","shell.execute_reply.started":"2023-12-23T13:01:58.104229Z","shell.execute_reply":"2023-12-23T13:01:58.120047Z"},"trusted":true},"execution_count":null,"outputs":[]}]}