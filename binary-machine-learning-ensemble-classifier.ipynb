{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sanprofnext/binary-machine-learning-ensemble-classifier?scriptVersionId=136156807\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"The code that follows deploys an ensemble learner comprising of a random forest classifier and a multi-layer neural network classifier to identify the likelihood of machine failures given a set of variable points associated with the machine and its environment\n\nThis code was built and run within the kaggle ecosystem for the kaggle challenge named **Binary Classification of Machine Failures** (https://www.kaggle.com/competitions/playground-series-s3e17/overview). Also, a tabular learner neural network from the fastai library was used to train the dataset and validate the results.\n\nThis approach is inspired from my learnings as a part of the fastai online training sessions. To test this file, you can open the challenge link, start a notebook and paste this code","metadata":{}},{"cell_type":"markdown","source":"# Data Ingestion and Quick Exploratory Analysis","metadata":{}},{"cell_type":"markdown","source":"1. What follows is a standard startup code snippet found across all Kaggle notebooks","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-08T15:28:48.133207Z","iopub.execute_input":"2023-07-08T15:28:48.133595Z","iopub.status.idle":"2023-07-08T15:28:48.18344Z","shell.execute_reply.started":"2023-07-08T15:28:48.133564Z","shell.execute_reply":"2023-07-08T15:28:48.18171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Import all the required libraries. Please note the presence of fastai library in order to generate a neural network based tabular learner","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nfrom sklearn.ensemble import RandomForestClassifier\nfrom fastai.tabular.all import *\nfrom sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2023-07-08T15:28:48.18644Z","iopub.execute_input":"2023-07-08T15:28:48.186976Z","iopub.status.idle":"2023-07-08T15:28:53.103694Z","shell.execute_reply.started":"2023-07-08T15:28:48.186928Z","shell.execute_reply":"2023-07-08T15:28:53.102383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = Path('/kaggle/input/playground-series-s3e17')\n! ls {path}","metadata":{"execution":{"iopub.status.busy":"2023-07-08T15:28:53.105117Z","iopub.execute_input":"2023-07-08T15:28:53.105781Z","iopub.status.idle":"2023-07-08T15:28:54.14966Z","shell.execute_reply.started":"2023-07-08T15:28:53.105748Z","shell.execute_reply":"2023-07-08T15:28:54.148205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3. Read the training and test data sets along with capturing a quick snapshot of the value distribution across different predictors","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(path/'train.csv',low_memory=False)\ntest_df = pd.read_csv(path/'test.csv',low_memory=False)\ntrain_df.nunique()","metadata":{"execution":{"iopub.status.busy":"2023-07-08T15:28:54.153731Z","iopub.execute_input":"2023-07-08T15:28:54.154868Z","iopub.status.idle":"2023-07-08T15:28:54.739686Z","shell.execute_reply.started":"2023-07-08T15:28:54.154815Z","shell.execute_reply":"2023-07-08T15:28:54.738149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4. Detect the presence of missing entries or NaNs amongst the predictor variables","metadata":{}},{"cell_type":"code","source":"train_df.isna().sum()\ntest_df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-07-08T15:28:54.74168Z","iopub.execute_input":"2023-07-08T15:28:54.742234Z","iopub.status.idle":"2023-07-08T15:28:54.836523Z","shell.execute_reply.started":"2023-07-08T15:28:54.74219Z","shell.execute_reply":"2023-07-08T15:28:54.834748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The predictor space is fairly clean at this point in time given there are no missing entries","metadata":{}},{"cell_type":"markdown","source":"5. The id predictor can be discarded from both the training and test sets as it just represents a unique id for each instance","metadata":{}},{"cell_type":"code","source":"train_df.drop('id',inplace=True,axis='columns')\ntest_df.drop('id',inplace=True,axis='columns')","metadata":{"execution":{"iopub.status.busy":"2023-07-08T15:28:54.849761Z","iopub.execute_input":"2023-07-08T15:28:54.850385Z","iopub.status.idle":"2023-07-08T15:28:54.88365Z","shell.execute_reply.started":"2023-07-08T15:28:54.850346Z","shell.execute_reply":"2023-07-08T15:28:54.882396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"6. It is worthwhile to understand the performance of a null classifier which assumes the predicted class across all training instances to be the most frequently occuring class label","metadata":{}},{"cell_type":"code","source":"error_rate = train_df['Machine failure'].sum()/train_df['Machine failure'].shape[0]\nnull_accuracy = 1- error_rate\nnull_accuracy","metadata":{"execution":{"iopub.status.busy":"2023-07-08T15:28:54.885407Z","iopub.execute_input":"2023-07-08T15:28:54.886182Z","iopub.status.idle":"2023-07-08T15:28:54.896045Z","shell.execute_reply.started":"2023-07-08T15:28:54.88612Z","shell.execute_reply":"2023-07-08T15:28:54.894694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The misclassification error would only be 1.58% (98.4% accuracy) even with a null classifier.Hence, it can be seen that the distribution of class labels across the binary categories is unbalanced which requires adapting the training strategy to avoid the associated biases","metadata":{}},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"1. On scrolling through the kaggle forums, the following features (predictors) wrapped in a function were engineered and added to the predictor space for helping improve the accuracy of predictions","metadata":{}},{"cell_type":"code","source":"def add_new_features(df):\n    df['Temperature_difference [K]'] = df['Air temperature [K]'] - df['Process temperature [K]']\n    df['Torque_speed_ratio'] =  df['Torque [Nm]']/df['Rotational speed [rpm]']\n    df['Energy'] = df['Rotational speed [rpm]']*df['Torque [Nm]']\n    df['Wear_rate'] = df['Tool wear [min]'] / df['Rotational speed [rpm]']\n    return df\n\ntrain_df = add_new_features(train_df)","metadata":{"execution":{"iopub.status.busy":"2023-07-08T15:28:54.89818Z","iopub.execute_input":"2023-07-08T15:28:54.898581Z","iopub.status.idle":"2023-07-08T15:28:54.917855Z","shell.execute_reply.started":"2023-07-08T15:28:54.898551Z","shell.execute_reply":"2023-07-08T15:28:54.916559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is a good practice to ensure that missing values are not introduced as NaNs when defining new predictors. On a quick check, it can be seen that irregularities are not introduced in the training data through addition of new predictors","metadata":{}},{"cell_type":"code","source":"train_df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-07-08T15:28:54.922189Z","iopub.execute_input":"2023-07-08T15:28:54.922594Z","iopub.status.idle":"2023-07-08T15:28:54.982586Z","shell.execute_reply.started":"2023-07-08T15:28:54.922564Z","shell.execute_reply":"2023-07-08T15:28:54.981492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A similar test can be performed on the test data to ensure that missing values do not surface during feature engineering","metadata":{}},{"cell_type":"code","source":"test_df = add_new_features(test_df)\ntest_df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-07-08T15:28:54.984057Z","iopub.execute_input":"2023-07-08T15:28:54.984428Z","iopub.status.idle":"2023-07-08T15:28:55.033162Z","shell.execute_reply.started":"2023-07-08T15:28:54.984398Z","shell.execute_reply":"2023-07-08T15:28:55.031692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Additional sanitization can be performed by eliminating duplicate entries from the training dataset","metadata":{}},{"cell_type":"code","source":"dup_instances = train_df.duplicated()\nif dup_instances.value_counts()[1]>0: train_df.drop_duplicates(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-08T15:28:55.035409Z","iopub.execute_input":"2023-07-08T15:28:55.035895Z","iopub.status.idle":"2023-07-08T15:28:55.295502Z","shell.execute_reply.started":"2023-07-08T15:28:55.035863Z","shell.execute_reply":"2023-07-08T15:28:55.293718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Identifying Significant predictors through a random forest classifier","metadata":{}},{"cell_type":"markdown","source":"This section involves creation of a tabular data structure using the fastai library and then extracting the training and validation sets from the structure for training a random forest classifier. The goal at this stage is not to predict the likelihood of machine failure, but to identify the top predictors in the predictor space in terms of association with the failure event","metadata":{}},{"cell_type":"markdown","source":"1. The predictors are identified as being continuous vs. categorical as fastai tabular dataloader requires this piece of information. The cont_cat_split function from the fastai library is used for this purpose which results in the names of continuous predictors being stored in cont_names and the names of categorical predictors being stored in cat_names","metadata":{}},{"cell_type":"code","source":"dep_var = 'Machine failure'\ntrain_df['Product ID'] = train_df['Product ID'].astype('category')\ncont_names,cat_names = cont_cat_split(train_df,dep_var = dep_var)","metadata":{"execution":{"iopub.status.busy":"2023-07-08T15:28:55.297508Z","iopub.execute_input":"2023-07-08T15:28:55.298063Z","iopub.status.idle":"2023-07-08T15:28:55.341724Z","shell.execute_reply.started":"2023-07-08T15:28:55.297973Z","shell.execute_reply":"2023-07-08T15:28:55.34Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. A tabular pandas object is created by leveraging the fastai library while subjecting the features to categorification. It should be noted that stratified sampling is used for achieving the split between the training and validation sets so that the ratio of failure and non-failure samples is consistent across the training and validation sets","metadata":{"execution":{"iopub.status.busy":"2023-06-18T14:51:43.56137Z","iopub.execute_input":"2023-06-18T14:51:43.561758Z","iopub.status.idle":"2023-06-18T14:51:43.569683Z","shell.execute_reply.started":"2023-06-18T14:51:43.561724Z","shell.execute_reply":"2023-06-18T14:51:43.568357Z"}}},{"cell_type":"code","source":"tp = TabularPandas(train_df,procs=[Categorify],cont_names=cont_names,cat_names=cat_names,\n                   y_names=dep_var,\n                   splits=TrainTestSplitter(test_size=0.25,stratify=train_df[dep_var].to_list())(range_of(train_df)))","metadata":{"execution":{"iopub.status.busy":"2023-07-08T15:28:55.343283Z","iopub.execute_input":"2023-07-08T15:28:55.344267Z","iopub.status.idle":"2023-07-08T15:28:55.625429Z","shell.execute_reply.started":"2023-07-08T15:28:55.344226Z","shell.execute_reply":"2023-07-08T15:28:55.624074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3. The training and validation data sets from the tabular pandas object are used to train a random forest classifier on the predictor space","metadata":{}},{"cell_type":"code","source":"train_x,train_y = tp.train.xs,tp.train.y\nvalid_x,valid_y = tp.valid.xs,tp.valid.y","metadata":{"execution":{"iopub.status.busy":"2023-07-08T15:28:55.627088Z","iopub.execute_input":"2023-07-08T15:28:55.627468Z","iopub.status.idle":"2023-07-08T15:28:55.651909Z","shell.execute_reply.started":"2023-07-08T15:28:55.627439Z","shell.execute_reply":"2023-07-08T15:28:55.650232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3. The model creation and training processes are wrapped in a function for reusability in the later sections. The **RandomForestClassifier class from the scikit-learn library** has been used here. It should be noted that the **'class weight' parameter is set to 'balanced'** so that the model does not get biased towards the excessive presence of non-failure class labels in the training set. The accuracy calculation is also wrapped in a function for reusability","metadata":{}},{"cell_type":"code","source":"def rf(xs,y,n_estimators=200,min_samples_leaf=20,max_features='sqrt',max_samples=0.7):\n    return RandomForestClassifier(n_jobs=-1,n_estimators=n_estimators,max_features=max_features,max_samples=max_samples,\n                                  min_samples_leaf=min_samples_leaf,oob_score=True,class_weight='balanced').fit(xs,y)\n\ndef m_accuracy(m,xs,y): return accuracy_score(y,m.predict(xs))","metadata":{"execution":{"iopub.status.busy":"2023-07-08T15:28:55.654108Z","iopub.execute_input":"2023-07-08T15:28:55.65449Z","iopub.status.idle":"2023-07-08T15:28:55.662339Z","shell.execute_reply.started":"2023-07-08T15:28:55.654458Z","shell.execute_reply":"2023-07-08T15:28:55.660377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4. The RandomForestClassifier is trained on the training data and the **performance is evaluated** based on **accuracy determined for the validation set and the out-of-bag training samples** corresponding to the decision tree ensemble","metadata":{}},{"cell_type":"code","source":"m = rf(train_x,train_y)\nprint(f'Accuracy:{m_accuracy(m,valid_x,valid_y)},OOB estimation score:{m.oob_score_}')","metadata":{"execution":{"iopub.status.busy":"2023-07-08T15:28:55.664042Z","iopub.execute_input":"2023-07-08T15:28:55.664544Z","iopub.status.idle":"2023-07-08T15:29:19.628192Z","shell.execute_reply.started":"2023-07-08T15:28:55.664508Z","shell.execute_reply":"2023-07-08T15:29:19.627189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It can be seen that the random forest model generates a higher accuracy compared to a null classifier. **Filtering out the weaker predictors** can **reduce model overfitting** and further improve the prediction accuracy","metadata":{}},{"cell_type":"markdown","source":"5. The **importance score** associated with each predictor can be identified using the **imp_scores function** post which the predictors with the least importance scores can be filtered out","metadata":{}},{"cell_type":"code","source":"def imp_scores(df,m):\n    return pd.DataFrame({'col':df.columns,'imp score':m.feature_importances_}).sort_values('imp score',ascending=False)\n\nimportance = imp_scores(train_x,m)\nimportance.plot('col','imp score','barh',xlabel='features',ylabel='importance score',legend=False,figsize=(10,10))","metadata":{"execution":{"iopub.status.busy":"2023-07-08T15:29:19.63004Z","iopub.execute_input":"2023-07-08T15:29:19.630474Z","iopub.status.idle":"2023-07-08T15:29:20.494888Z","shell.execute_reply.started":"2023-07-08T15:29:19.630443Z","shell.execute_reply":"2023-07-08T15:29:20.493219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importance","metadata":{"execution":{"iopub.status.busy":"2023-07-08T15:29:20.497054Z","iopub.execute_input":"2023-07-08T15:29:20.497792Z","iopub.status.idle":"2023-07-08T15:29:20.525888Z","shell.execute_reply.started":"2023-07-08T15:29:20.497753Z","shell.execute_reply":"2023-07-08T15:29:20.524062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"6. The predictors with importance score of >0.03 could be retained for future model fitting which will result in selection of ~top 80 %ile of predictors to reduce the possibility of overfitting","metadata":{}},{"cell_type":"code","source":"best_predictors = importance[importance['imp score']>0.03].col.to_list()\nbest_predictors","metadata":{"execution":{"iopub.status.busy":"2023-07-08T15:29:20.527968Z","iopub.execute_input":"2023-07-08T15:29:20.52843Z","iopub.status.idle":"2023-07-08T15:29:20.537679Z","shell.execute_reply.started":"2023-07-08T15:29:20.528394Z","shell.execute_reply":"2023-07-08T15:29:20.536566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest Classifier prediction on the selected best predictors","metadata":{}},{"cell_type":"markdown","source":"1. Identify validation set prediction accuracy and out-of-bag prediction accuracy with the random forest classifier trained on a predictor space consisting of only the best predictors identified in the previous section","metadata":{"execution":{"iopub.status.busy":"2023-06-18T17:23:32.235286Z","iopub.execute_input":"2023-06-18T17:23:32.235708Z","iopub.status.idle":"2023-06-18T17:23:32.243607Z","shell.execute_reply.started":"2023-06-18T17:23:32.235675Z","shell.execute_reply":"2023-06-18T17:23:32.242348Z"}}},{"cell_type":"code","source":"train_x_best, valid_x_best = tp.train.xs[best_predictors],tp.valid.xs[best_predictors]\nm_best_pred = rf(train_x_best,train_y)\nprint(f'Accuracy:{m_accuracy(m_best_pred,valid_x_best,valid_y)},OOB estimation score:{m_best_pred.oob_score_}')","metadata":{"execution":{"iopub.status.busy":"2023-07-08T15:29:20.539192Z","iopub.execute_input":"2023-07-08T15:29:20.540121Z","iopub.status.idle":"2023-07-08T15:29:39.994277Z","shell.execute_reply.started":"2023-07-08T15:29:20.540089Z","shell.execute_reply":"2023-07-08T15:29:39.991845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The classification accuracy more or less stays the same as that of the full predictor space,hence the best predictors determined in the previous section can be used in the training iterations henceforth to foreclose any possibility of model overfitting","metadata":{}},{"cell_type":"markdown","source":"# Neural network classifier prediction on the selected best predictors","metadata":{}},{"cell_type":"markdown","source":"1. A separate set of **training and validation datasets consisting of only identified best predictors** from the previous sections could be used to train a **neural network classifier**. The continuous and categorical columns can be extracted at this step as well","metadata":{}},{"cell_type":"code","source":"dep_var_nn = 'Machine failure'\ntrain_df_nn, test_df_nn = train_df[best_predictors+[dep_var_nn]],test_df[best_predictors]\ncont_nn,cat_nn = cont_cat_split(train_df_nn,dep_var=dep_var_nn)","metadata":{"execution":{"iopub.status.busy":"2023-07-08T15:29:39.996048Z","iopub.execute_input":"2023-07-08T15:29:39.997415Z","iopub.status.idle":"2023-07-08T15:29:40.040485Z","shell.execute_reply.started":"2023-07-08T15:29:39.997361Z","shell.execute_reply":"2023-07-08T15:29:40.039003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. A **tabular dataloader** to feed the training and validation data into a fastai neural network learner can be created at this stage from a tabular pandas object that was used earlier. A couple of quick callouts here:\n    * The **predictors should be subjected to normalization** for training a neural network as the model is sensitive to the relative scale variation across predictors\n    * A **batch of 1024** has been selected for each training step of the neural network in an epoch","metadata":{}},{"cell_type":"code","source":"tp_dl = TabularPandas(train_df_nn,procs=[Categorify,Normalize],cont_names = cont_nn,cat_names=cat_nn,y_names=dep_var_nn,\n                     splits=TrainTestSplitter(test_size=0.25,stratify=train_df_nn[dep_var].to_list())(range_of(train_df_nn)),\n                      y_block=CategoryBlock()).dataloaders(bs=1024,path=\".\")\ntp_dl.show(3)","metadata":{"execution":{"iopub.status.busy":"2023-07-08T15:29:40.04201Z","iopub.execute_input":"2023-07-08T15:29:40.042407Z","iopub.status.idle":"2023-07-08T15:29:40.428873Z","shell.execute_reply.started":"2023-07-08T15:29:40.042374Z","shell.execute_reply":"2023-07-08T15:29:40.427578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3. A tabular learner built using fastai library can set up the neural network architecture for training on the task of machine failure likelihood prediction. The layers argument can be experimented with further to achieve higher accuracy on the validation set","metadata":{}},{"cell_type":"code","source":"learner = tabular_learner(tp_dl,layers=[256,128],metrics=accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-07-08T15:29:40.432859Z","iopub.execute_input":"2023-07-08T15:29:40.433341Z","iopub.status.idle":"2023-07-08T15:29:40.579116Z","shell.execute_reply.started":"2023-07-08T15:29:40.433305Z","shell.execute_reply":"2023-07-08T15:29:40.57799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4. The fastai library has a function through which the learning rate can be determined for the various training steps","metadata":{}},{"cell_type":"code","source":"learner.lr_find(suggest_funcs=(slide,valley))","metadata":{"execution":{"iopub.status.busy":"2023-07-08T15:29:40.580878Z","iopub.execute_input":"2023-07-08T15:29:40.58134Z","iopub.status.idle":"2023-07-08T15:29:47.336617Z","shell.execute_reply.started":"2023-07-08T15:29:40.581307Z","shell.execute_reply":"2023-07-08T15:29:47.335505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"5. The model is trained with a weight decay parameter (wd) for regularization to prevent overfitting. The learning rate corresponding to the slide data point can be used for fine-tuning the model","metadata":{}},{"cell_type":"code","source":"learner.fit_one_cycle(5,1e-1,wd=0.1)","metadata":{"execution":{"iopub.status.busy":"2023-07-08T15:29:47.338288Z","iopub.execute_input":"2023-07-08T15:29:47.338646Z","iopub.status.idle":"2023-07-08T15:30:18.444703Z","shell.execute_reply.started":"2023-07-08T15:29:47.338615Z","shell.execute_reply":"2023-07-08T15:30:18.443178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model accuracy is much better than that seen with the random forest classifier. Hence, an ensemble of predictions from both the models can also be experimented with","metadata":{}},{"cell_type":"markdown","source":"# Ensembling predictions from the random forest classifier and the neural network learner along with testing the same on the validation data set","metadata":{}},{"cell_type":"markdown","source":"1. A new random forest classifier is trained on a similar predictor space as that supplied to the neural network classifier (tabular learner) to ensure that the same validation set can be used for model performance ensembling of both the approaches. The ensemble predictions accuracy of failure likelihood from both the approaches can be determined at this point in time","metadata":{}},{"cell_type":"code","source":"tp_rf = TabularPandas(train_df_nn,procs=[Categorify],cont_names = cont_nn,cat_names=cat_nn,y_names=dep_var_nn,\n                     splits=TrainTestSplitter(test_size=0.25,stratify=train_df_nn[dep_var].to_list())(range_of(train_df_nn)),\n                      ).dataloaders(path=\".\")\nm_rf = rf(tp_rf.train.xs,tp_rf.train.y)\nrf_prob = m_rf.predict_proba(tp_rf.valid.xs)\ntl_probs,_ = learner.get_preds(dl=learner.dls.valid)\nensemble_prob = (rf_prob[:,1]+to_np(tl_probs[:,1].squeeze()))/2\naccuracy_score(learner.dls.valid.y,np.where(ensemble_prob>0.5,1,0))","metadata":{"execution":{"iopub.status.busy":"2023-07-08T15:30:18.446732Z","iopub.execute_input":"2023-07-08T15:30:18.447103Z","iopub.status.idle":"2023-07-08T15:30:40.197926Z","shell.execute_reply.started":"2023-07-08T15:30:18.447074Z","shell.execute_reply":"2023-07-08T15:30:40.196714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It can be seen that the accuracy score computed on the validation data set for the ensemble is better than that of the null classifier or that of the individual classifier models","metadata":{}},{"cell_type":"markdown","source":"# Training the model on the entire training data set","metadata":{}},{"cell_type":"markdown","source":"1. Before being applied on a test/holdout set, the ensemble model that has now been validated for returning the best accuracy can be trained on the entire training dataset","metadata":{}},{"cell_type":"code","source":"tp_full_rf = TabularPandas(train_df_nn,procs=[Categorify],cont_names = cont_nn,cat_names=cat_nn,y_names=dep_var_nn,\n                     splits=MaskSplitter([False]*train_df_nn.shape[0])(range_of(train_df_nn))).dataloaders(path='.')\ntp_full_nn = TabularPandas(train_df_nn,procs=[Categorify,Normalize],cont_names = cont_nn,cat_names=cat_nn,y_names=dep_var_nn,\n                     splits=MaskSplitter([False]*train_df_nn.shape[0])(range_of(train_df_nn)),\n                      y_block=CategoryBlock()).dataloaders(bs=1024,path=\".\")\nrf_full = rf(tp_full_rf.train.xs,tp_full_rf.train.y)\nlearner_full = tabular_learner(tp_full_nn,layers=[256,128])\nlearner_full.fit_one_cycle(5,1e-1,wd=0.1)","metadata":{"execution":{"iopub.status.busy":"2023-07-08T15:30:40.202829Z","iopub.execute_input":"2023-07-08T15:30:40.203453Z","iopub.status.idle":"2023-07-08T15:31:48.76524Z","shell.execute_reply.started":"2023-07-08T15:30:40.203416Z","shell.execute_reply":"2023-07-08T15:31:48.762879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. A method like the one shown below can be used to apply the latest fine-tuned model on a test/holdout data set and return a data frame of the likelihood predictions","metadata":{}},{"cell_type":"code","source":"def create_submission_ensemble(rf_m,rf_tp,learner):\n    final_test_df = pd.read_csv(path/'test.csv',low_memory=False)\n    test_dls_nn = learner.dls.test_dl(test_df_nn)\n    preds_nn,_ = learner.get_preds(dl=test_dls_nn)\n    test_dls_rf = rf_tp.test_dl(test_df_nn)\n    preds_rf = rf_m.predict_proba(test_dls_rf.xs)\n    ensemble_pred = (preds_rf[:,1] + to_np(preds_nn[:,1].squeeze()))/2\n    final_test_df['Machine failure'] = ensemble_pred\n    subm = final_test_df[['id','Machine failure']]\n    return subm","metadata":{"execution":{"iopub.status.busy":"2023-07-08T15:31:48.768611Z","iopub.execute_input":"2023-07-08T15:31:48.769193Z","iopub.status.idle":"2023-07-08T15:31:48.778423Z","shell.execute_reply.started":"2023-07-08T15:31:48.769112Z","shell.execute_reply":"2023-07-08T15:31:48.776731Z"},"trusted":true},"execution_count":null,"outputs":[]}]}