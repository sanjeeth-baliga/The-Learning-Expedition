{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7315935,"sourceType":"datasetVersion","datasetId":4245390},{"sourceId":7332086,"sourceType":"datasetVersion","datasetId":4256318},{"sourceId":7355516,"sourceType":"datasetVersion","datasetId":4272020},{"sourceId":7361913,"sourceType":"datasetVersion","datasetId":4276415}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## The Outside-in Analysis Assistant","metadata":{}},{"cell_type":"markdown","source":"For this application, the langchain framework is leveraged for running Retrieval Augmented Generation (RAG) on the annual report of a target company to extract the challenges faced along specific dimensions and the strategic business priorities along specific dimensions. Together, these inform the competencies that need to be cultivated in the company to sustain a strategic advantage in the market","metadata":{}},{"cell_type":"markdown","source":"There are two categories of LLM chains involved here\n* The first category of LLM chains built using Langchain Expression Language (LCEL) to extract relevant data points from the vectorized company annual report with langchain functional APIs (called as extraction chains)\n* The second category of LLM chains to weave the extracted data in a suitable structure for executive consumption and also identify the potential business competencies required","metadata":{}},{"cell_type":"markdown","source":"The results generated by the workflow are captured in the following queries\n* Business challenges: **Variable** - b_challenges\n* Strategic business priorities: **Variable** - b_priorities\n* Competency: **Variable** - potential_competency","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-08T17:37:15.622672Z","iopub.execute_input":"2024-01-08T17:37:15.623100Z","iopub.status.idle":"2024-01-08T17:37:16.060180Z","shell.execute_reply.started":"2024-01-08T17:37:15.623067Z","shell.execute_reply":"2024-01-08T17:37:16.058739Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/annual-report-input/2022-sensata-ar.pdf\n/kaggle/input/example-drhp/Example_DRHP.pdf\n/kaggle/input/sample-drhp/imagine-marketing-limited-drhp.pdf\n/kaggle/input/annual-input-report/Microchip_Annual.pdf\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Install the required modules and import the required libraries","metadata":{}},{"cell_type":"code","source":"#!pip install --upgrade pip\n!pip install typing_extensions openai langchain --quiet\n!pip install transformers --quiet\n!pip install InstructorEmbedding --quiet\n!pip install chromadb --quiet\n!pip install sentence-transformers --quiet\n!pip uninstall typing_extensions --yes --quiet\n!pip uninstall openai --yes --quiet\n!pip install typing_extensions==3.10.0.2 openai==0.28.0 --quiet","metadata":{"execution":{"iopub.status.busy":"2024-01-08T17:37:16.062087Z","iopub.execute_input":"2024-01-08T17:37:16.062640Z","iopub.status.idle":"2024-01-08T17:39:25.372205Z","shell.execute_reply.started":"2024-01-08T17:37:16.062601Z","shell.execute_reply":"2024-01-08T17:39:25.370676Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.0.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.1 requires jupyterlab<5.0.0a0,>=4.0.6, but you have jupyterlab 4.0.5 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.4 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.9.0 which is incompatible.\ntensorflow-probability 0.21.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\nydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\ngoogle-cloud-pubsublite 1.8.3 requires overrides<7.0.0,>=6.0.1, but you have overrides 7.4.0 which is incompatible.\njupyterlab 4.0.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.1 requires jupyterlab<5.0.0a0,>=4.0.6, but you have jupyterlab 4.0.5 which is incompatible.\nkfp 2.0.1 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.0.1 requires kubernetes<27,>=8.0.0, but you have kubernetes 28.1.0 which is incompatible.\nopentelemetry-exporter-otlp 1.19.0 requires opentelemetry-exporter-otlp-proto-grpc==1.19.0, but you have opentelemetry-exporter-otlp-proto-grpc 1.22.0 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.19.0 requires opentelemetry-exporter-otlp-proto-common==1.19.0, but you have opentelemetry-exporter-otlp-proto-common 1.22.0 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.19.0 requires opentelemetry-proto==1.19.0, but you have opentelemetry-proto 1.22.0 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.19.0 requires opentelemetry-sdk~=1.19.0, but you have opentelemetry-sdk 1.22.0 which is incompatible.\ntensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.7.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsqlalchemy 2.0.20 requires typing-extensions>=4.2.0, but you have typing-extensions 3.10.0.2 which is incompatible.\nalembic 1.13.0 requires typing-extensions>=4, but you have typing-extensions 3.10.0.2 which is incompatible.\naltair 5.2.0 requires typing-extensions>=4.0.1; python_version < \"3.11\", but you have typing-extensions 3.10.0.2 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.1 which is incompatible.\nasgiref 3.7.2 requires typing-extensions>=4; python_version < \"3.11\", but you have typing-extensions 3.10.0.2 which is incompatible.\nastroid 3.0.2 requires typing-extensions>=4.0.0; python_version < \"3.11\", but you have typing-extensions 3.10.0.2 which is incompatible.\nasync-lru 2.0.4 requires typing-extensions>=4.0.0; python_version < \"3.11\", but you have typing-extensions 3.10.0.2 which is incompatible.\nchex 0.1.85 requires typing-extensions>=4.2.0, but you have typing-extensions 3.10.0.2 which is incompatible.\nchromadb 0.4.22 requires typing-extensions>=4.5.0, but you have typing-extensions 3.10.0.2 which is incompatible.\ncloudpathlib 0.16.0 requires typing_extensions>4; python_version < \"3.11\", but you have typing-extensions 3.10.0.2 which is incompatible.\nfastapi 0.101.1 requires typing-extensions>=4.5.0, but you have typing-extensions 3.10.0.2 which is incompatible.\nflax 0.7.5 requires typing-extensions>=4.2, but you have typing-extensions 3.10.0.2 which is incompatible.\ngymnasium 0.29.0 requires typing-extensions>=4.3.0, but you have typing-extensions 3.10.0.2 which is incompatible.\njupyterlab 4.0.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.1 requires jupyterlab<5.0.0a0,>=4.0.6, but you have jupyterlab 4.0.5 which is incompatible.\nlibrosa 0.10.1 requires typing-extensions>=4.1.1, but you have typing-extensions 3.10.0.2 which is incompatible.\npydantic 1.10.12 requires typing-extensions>=4.2.0, but you have typing-extensions 3.10.0.2 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.4 which is incompatible.\npython-utils 3.8.1 requires typing-extensions>3.10.0.2, but you have typing-extensions 3.10.0.2 which is incompatible.\npytorch-lightning 2.1.2 requires typing-extensions>=4.0.0, but you have typing-extensions 3.10.0.2 which is incompatible.\nuvicorn 0.23.2 requires typing-extensions>=4.0; python_version < \"3.11\", but you have typing-extensions 3.10.0.2 which is incompatible.\nydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from langchain.prompts import ChatPromptTemplate\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.vectorstores.chroma import Chroma\nfrom langchain.embeddings import HuggingFaceInstructEmbeddings\nfrom langchain.schema.output_parser import StrOutputParser\nfrom langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\nfrom typing import List\nfrom pydantic import BaseModel,Field\nfrom langchain.utils.openai_functions import convert_pydantic_to_openai_function\nfrom langchain.chains import SequentialChain\nfrom langchain.chains import LLMChain\nimport transformers\nimport torch\nimport json\nimport openai\nimport time","metadata":{"execution":{"iopub.status.busy":"2024-01-08T17:39:25.374258Z","iopub.execute_input":"2024-01-08T17:39:25.374693Z","iopub.status.idle":"2024-01-08T17:39:32.845047Z","shell.execute_reply.started":"2024-01-08T17:39:25.374653Z","shell.execute_reply":"2024-01-08T17:39:32.843777Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Use the PyPDFLoader library to parse content from the PDF annual report and store it in Chroma vector database for retrieval with queries","metadata":{}},{"cell_type":"code","source":"loader = PyPDFLoader('/kaggle/input/example-drhp/Example_DRHP.pdf')\npages = loader.load_and_split()\n\n#Set the start_page and end_page as user inputs to extract the relevant document pages for vectorization\nstart_page = 57\nend_page = 86\nrelevant_pages = list()\nrelevant_pages = [page for page in pages if page.metadata['page']>=start_page and page.metadata['page']<=end_page]\n\n#Use the sentence-transformer based embeddings for vector representation of content to store in the Chroma database\nembedding_routine = HuggingFaceInstructEmbeddings()\n\n#Ensure that the relevant page sections are strored in the vector database with the embeddings generator\nvectordb = Chroma.from_documents(documents=relevant_pages,embedding=embedding_routine)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T17:39:32.848231Z","iopub.execute_input":"2024-01-08T17:39:32.848979Z","iopub.status.idle":"2024-01-08T17:43:43.066437Z","shell.execute_reply.started":"2024-01-08T17:39:32.848935Z","shell.execute_reply":"2024-01-08T17:43:43.064355Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":".gitattributes:   0%|          | 0.00/1.48k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d044266895743afa079f7b6a47a22a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/270 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d847558445684621b2bfe4018577ab2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"2_Dense/config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e15cb124cf8246ca844ddebde8eb0a2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/3.15M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"221ef1a951134f80803ea9d4972fc544"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/66.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af2e46601905443cb5738e66d72baac1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78f245f41a8b418aa099133caaaf40b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14ca8f8eae8346f18d9de6e00efa2ea9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.34G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c07ae86b8e2438dad9895cab9cea0cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca5b682fddf04c83aefc542e56fb1459"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08f201de063444d6be27ddca5e7239ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2de62924806c4d55b2b562fb207c0039"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19a6500939494526b9f0cb50db281614"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.41k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"960db32d2e2c426aa416deb4901379c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/461 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a776c99758814739894ab82f0a817fe4"}},"metadata":{}},{"name":"stdout","text":"load INSTRUCTOR_Transformer\nmax_seq_length  512\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Define pydantic classes with field values to be extracted from the vector database. These classes will be translated into openai native functions that can be plugged into LLM chains for retrieval of specific data pieces","metadata":{}},{"cell_type":"code","source":"class Challenge(BaseModel):\n    \"\"\"Detailed information of challenges and constraints faced by the company that hinder business value. These are rich illustrations of specific issues relevant to the company and not generic statements from field description. If nothing is found, please mention not found and do not make up things\"\"\"\n    customers: str = Field(description=\"Anything related to customer demand patterns and changes in customer preferences\")\n    competition: str = Field(description=\"Anything related to competitive pressure and substitutes in the market\")\n    production_factors: str = Field(description=\"Anything related to shortages or ineffective deployment of factors of production or service delivery\")\n    procurement: str = Field(description=\"Anything related to supply side constraints in procurement of raw materials or services\")\n    supply_chain: str = Field(description=\"Anything related to supply chain constraints, labor constraints, or organization model constraints\")\n    process_inefficiency: str = Field(description=\"Anything related to process inefficiencies and functional silos across departments\")\n        \nclass StratPrior(BaseModel):\n    \"\"\"Detailed information on strategic business priorities of the company. These are rich illustrations of areas specific to the company and not generic statements. If nothing is found, please mention not found and do not make up things\"\"\"\n    new_product: str = Field(description=\"Details on new product or services targeted for growth\")\n    new_market: str = Field(description=\"Details on potential new market targets for growth acceleration\")\n    efficiency: str = Field(description=\"Details on efficiency improvements targeted for internal operations\")\n    digitization: str = Field(description=\"Details on initiatives identified for digitization and analytics\")\n    technology: str = Field(description=\"Details on requirements for a change in technological landscape\")\n    operating_model: str = Field(description=\"Details on planned interventions for operating model transformation\")","metadata":{"execution":{"iopub.status.busy":"2024-01-08T17:43:43.070066Z","iopub.execute_input":"2024-01-08T17:43:43.071962Z","iopub.status.idle":"2024-01-08T17:43:43.087645Z","shell.execute_reply.started":"2024-01-08T17:43:43.071901Z","shell.execute_reply":"2024-01-08T17:43:43.086777Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Translate the pydantic classes into langchain API functions and bind them with the instance of a ChatOpenAI model to extract the challenges and strategic priorities of the target company from the vector database","metadata":{}},{"cell_type":"code","source":"fn_extracts = [\n    convert_pydantic_to_openai_function(f) for f in [\n        Challenge, StratPrior\n    ]\n]\n\n#Instantiate an OpenAI endpoint powered by GPT3.5 LLM for running the queries encapsulated in the langchain functions \n#on the retrieved data from the vector database\nmodel = ChatOpenAI(model_name='gpt-3.5-turbo',temperature=0.05,openai_api_key='sk-****************************************')\n\n#Bind the langchain API functions with the model to inform the retrieval of relevant data points \ninfo_extractor = model.bind(functions=fn_extracts)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T17:43:43.089186Z","iopub.execute_input":"2024-01-08T17:43:43.089857Z","iopub.status.idle":"2024-01-08T17:43:43.590038Z","shell.execute_reply.started":"2024-01-08T17:43:43.089825Z","shell.execute_reply":"2024-01-08T17:43:43.589093Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:115: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use langchain_openai.ChatOpenAI instead.\n  warn_deprecated(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The prompt template here is desiged for insertion into the LLM chain. The query parameter of the prompt determines the langchain function that should be invoked for retrieving the relevant data pieces from the vector database ","metadata":{}},{"cell_type":"code","source":"instruction = \"\"\"Please respond to questions on the business areas based on the reference context. If the answer is not known, do not mention the field description in the response\"\"\"\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\",instruction),\n    (\"user\",\"{query} answered by using:{context}\")\n])","metadata":{"execution":{"iopub.status.busy":"2024-01-08T17:43:43.591223Z","iopub.execute_input":"2024-01-08T17:43:43.592082Z","iopub.status.idle":"2024-01-08T17:43:43.598963Z","shell.execute_reply.started":"2024-01-08T17:43:43.592047Z","shell.execute_reply":"2024-01-08T17:43:43.597567Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Define helper function that feed the output generated by one chain in the right format to the prompt template of the subsequent chains","metadata":{}},{"cell_type":"code","source":"#A json cleanup function that solves for the common errors faced during generation of json-formatted string by OpenAI LLM\n#This is inserted as the last step in an LLM chain to translate LLM-generated output in json format after performing a few validation checks\ndef clean_str_for_json(in_str):\n    str_output = in_str.additional_kwargs['function_call']['arguments']\n    try:\n        formt_json = json.loads(str_output)\n        return formt_json\n    except ValueError as e:\n        try:\n            formt_json = json.loads(str_output+\"\\\"\\n}\")\n            return formt_json\n        except ValueError as e:\n            return json.loads(str_output[:-3]+\"}\")\n\n\n#Use the function to combine values corresponding to common keys generated as json object from LLM extraction chain \n#This is used to handle the limitation posed by OpenAI APIs which can process only 3 queries per minute and parse the vectorized data in chunks of 3 blocks each. \ndef combine_content_with_common_key(list_input):\n    #combined_out = {key:\".\".join([item[key] for item in list_input if item[key] not in ['Not found']]) for key in list_input[0].keys()}\n    combined_out = {k:\".\".join([d.get(k) for d in list_input if k in d]) for k in set().union(*list_input)}\n    for key in combined_out.keys():\n        if combined_out[key] == '':\n            combined_out.pop(key)\n    return combined_out\n\n#This function and the next are used to convert json objects to strings for plugging into prompt templates\ndef concat_dict(gen_output):\n    out_text = \"\".join([key+\":\"+val for key,val in gen_output.items() if val is not None])\n    return out_text\n\ndef compile_para(json_text):\n    return \".\".join([val for val in json_text.values() if val not in ['None']])\n\n#Adds a delay of 60 seconds to overcome the query limits poased by OpenAI \ndef delay():\n    time.sleep(60)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T17:54:42.248640Z","iopub.execute_input":"2024-01-08T17:54:42.249255Z","iopub.status.idle":"2024-01-08T17:54:42.261295Z","shell.execute_reply.started":"2024-01-08T17:54:42.249215Z","shell.execute_reply":"2024-01-08T17:54:42.259841Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Using Langchain Expression Language (LCEL), build a customized data extraction chain with the prompt, function-embedded retriever model and the json cleanup function. This chain can perform the required operation based on the invocation and return the result in json format","metadata":{}},{"cell_type":"code","source":"extraction_chain = prompt | info_extractor | clean_str_for_json","metadata":{"execution":{"iopub.status.busy":"2024-01-08T17:43:43.614278Z","iopub.execute_input":"2024-01-08T17:43:43.614703Z","iopub.status.idle":"2024-01-08T17:43:43.632604Z","shell.execute_reply.started":"2024-01-08T17:43:43.614675Z","shell.execute_reply":"2024-01-08T17:43:43.631357Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Extract the relevant content sections by running a semantic search with the query on the vector database. Invoke the extraction chain query on the retrieved content sections to pull out the company challenges in json format","metadata":{}},{"cell_type":"code","source":"query_pp = \"Challenges and constraints related to market, customers, competition, suppliers or internal processes\"\ncontext_pp = vectordb.similarity_search(query=query_pp,k=27)\nchallenges = list()\nfor tr in range(3):\n    challenges += [extraction_chain.invoke({\"query\":query_pp,\"context\":context_pp[tr*9+i*3:tr*9+i*3+3]}) for i in range(3)]\n    delay()\nconct_challenges = combine_content_with_common_key(challenges)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T17:54:45.562474Z","iopub.execute_input":"2024-01-08T17:54:45.563826Z","iopub.status.idle":"2024-01-08T17:58:25.848903Z","shell.execute_reply.started":"2024-01-08T17:54:45.563772Z","shell.execute_reply":"2024-01-08T17:58:25.846942Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Extract the relevant content sections by running a semantic search with the query on the vector database. Invoke the extraction chain query on the retrieved content sections to pull out the company challenges in json format","metadata":{}},{"cell_type":"code","source":"query_stimp = \"Strategic priorities and initiatives envisioned by the company related to product, market, internal processes or capability development\"\ncontext_stimp = vectordb.similarity_search(query=query_stimp,k=9)\nsb_priorities = list()\nfor tr in range(3):\n    sb_priorities += [extraction_chain.invoke({\"query\":query_stimp,\"context\":context_stimp[tr*9+i*3:tr*9+i*3+3]}) for i in range(3)]\n    delay()\nconct_sb_priorities = combine_content_with_common_key(sb_priorities)","metadata":{"execution":{"iopub.status.busy":"2024-01-08T17:58:32.950123Z","iopub.execute_input":"2024-01-08T17:58:32.950592Z","iopub.status.idle":"2024-01-08T18:02:05.712598Z","shell.execute_reply.started":"2024-01-08T17:58:32.950558Z","shell.execute_reply":"2024-01-08T18:02:05.710924Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Create separate llm chains to parse the retrieved data extracts by the extraction chains and create professional content suitable for the consumption of executives with respect to the business challenges and strategic priorities","metadata":{}},{"cell_type":"code","source":"interpret_model = ChatOpenAI(model_name='gpt-3.5-turbo',temperature=0.05,openai_api_key='sk-sk-****************************************')\npp_prompt = ChatPromptTemplate.from_template(\n    \"Summarize the specific challenges of the company in one or two sentences based on the reference text in triple backticks. Please avoid generic statements, be as specific as possible and eliminate duplication. Please lay out challenge details in crisp format and structure response as JSON\\n```{challenges}```\"\n)\nchain_pp = LLMChain(llm=interpret_model,prompt=pp_prompt)\nb_challenges = json.loads(chain_pp.run(concat_dict(conct_challenges)))","metadata":{"execution":{"iopub.status.busy":"2024-01-08T18:02:05.715863Z","iopub.execute_input":"2024-01-08T18:02:05.716380Z","iopub.status.idle":"2024-01-08T18:02:11.378038Z","shell.execute_reply.started":"2024-01-08T18:02:05.716336Z","shell.execute_reply":"2024-01-08T18:02:11.376809Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:115: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n  warn_deprecated(\n","output_type":"stream"}]},{"cell_type":"code","source":"sb_prompt = ChatPromptTemplate.from_template(\n    \"Summarize the specific business priorities of the company in one or two sentences based on the reference text in triple backticks. Please avoid generic statements, be as specific as possible and eliminate duplication. Please lay out priority details in crisp format and structure response as JSON\\n```{sb_priorities}```\"\n)\nchain_sb = LLMChain(llm=interpret_model,prompt=sb_prompt)\nb_priorities = json.loads(chain_sb.run(concat_dict(conct_sb_priorities)))","metadata":{"execution":{"iopub.status.busy":"2024-01-08T18:02:11.379234Z","iopub.execute_input":"2024-01-08T18:02:11.379569Z","iopub.status.idle":"2024-01-08T18:02:16.114508Z","shell.execute_reply.started":"2024-01-08T18:02:11.379542Z","shell.execute_reply":"2024-01-08T18:02:16.113252Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Create an LLM chain comprising of a prompt and an LLM model that processes the business challenges and business priorities generated by the prior chains to generate recommendations on the competencies that the business should focus on developing ","metadata":{}},{"cell_type":"code","source":"comp_prompt = ChatPromptTemplate.from_template(\n    \"\"\"Identify the differentiating competencies that need to be developed by the company. A competency is a specific area of expertise that would help improve product, brand or internal operations developed through the right alignment of system, process reinvention and operating model transformation. \n    A competency is not an initiative. The competencies should be able to solve the challenges enclosed in ### and help achieve strategic business priorities enclosed in tripe backticks. \n    Please lay the response in json format having a crisp verbiage suitable for CXO consumption\\n###{b_challenges}###\\n```{b_priorities}```. \n    Each competency should be accompanied by short description as value of less than 30 words outlining the goal it would achieve. No need to list the challenges and business priorities separately. \n    Follow the unrelated example enclosed in $$$ for guidance with response enclosed in ***\n    $$$###Fragmented and adhoc reporting which is manually driven and incoherence between the insights across reports###\n    ```Simplified and systematic procedures for generating business insights to be consumed by business executives```$$$\n    ***Reporting factory:Trigger based generation of reports backed by reporting templates to simplify and consoilidate delivery of insights***\"\"\"\n)\nchain_comp = LLMChain(llm=interpret_model,prompt=comp_prompt)\ncompetency = json.loads(chain_comp.run({\"b_challenges\":compile_para(b_challenges),\"b_priorities\":compile_para(b_priorities)}))","metadata":{"execution":{"iopub.status.busy":"2024-01-08T18:02:16.117101Z","iopub.execute_input":"2024-01-08T18:02:16.117483Z","iopub.status.idle":"2024-01-08T18:02:28.742921Z","shell.execute_reply.started":"2024-01-08T18:02:16.117452Z","shell.execute_reply":"2024-01-08T18:02:28.741830Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"comp_dict = competency['competencies']\npotential_competency = {item['competency']:item['description'] for item in comp_dict}\npotential_competency","metadata":{"execution":{"iopub.status.busy":"2024-01-08T18:02:28.744020Z","iopub.execute_input":"2024-01-08T18:02:28.744385Z","iopub.status.idle":"2024-01-08T18:02:28.755948Z","shell.execute_reply.started":"2024-01-08T18:02:28.744354Z","shell.execute_reply":"2024-01-08T18:02:28.754274Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"{'Strengthen credit appraisal and risk management systems': 'Improve the ability to assess creditworthiness and manage risks associated with loans',\n 'Invest in technology systems and processes': 'Enhance operational and managerial efficiency through the adoption of advanced technology',\n 'Deploy strong technology systems': 'Enable swift response to market opportunities and challenges through robust technology infrastructure',\n 'Grow Gold loan business': \"Expand the company's gold loan portfolio to increase market share and revenue\",\n 'Explore opportunities in rural India': 'Tap into the potential of rural areas in India for gold loan financing',\n 'Implement technology-led processing systems': 'Improve efficiency and risk management capabilities through the adoption of technology-driven processing systems'}"},"metadata":{}}]}]}