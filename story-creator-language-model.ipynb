{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"The code that follows leverages language transformer models to perform a sequence of two tasks that enables the generation of a short story script in the english language\n1. **Semantic search** through a transformer fine-tuned to the sentence classification problem to extract the most relevant philosophical quote macthing an input key word from a larger corpus\n2. **Text generation** by fine tuning a transformer model to the sentence completion problem by leveraging the words generated from the earlier stage to start a new sentence\n\nThe huggingface model hub has been used to load the required datasets and fine-tune the transformer models for accomplishing the task described above. I would advise you to take a course offered by huggingfaces linked [here](https://huggingface.co/learn/nlp-course/chapter0/1?fw=pt) if you are interested in exploring this further or checking out other language models","metadata":{}},{"cell_type":"markdown","source":"This code has been built and tested in the kaggle ecosystem. Hence, if a similar notebook environment is utilized by a user, it is advisable to make all installations associated with libraries involving the huggingface models","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n!pip install datasets\n!pip install -U git+https://github.com/huggingface/transformers.git\n!pip install -U git+https://github.com/huggingface/accelerate.git\n!pip install faiss-cpu","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-04T17:54:09.905531Z","iopub.execute_input":"2023-07-04T17:54:09.906154Z","iopub.status.idle":"2023-07-04T17:54:09.931095Z","shell.execute_reply.started":"2023-07-04T17:54:09.906121Z","shell.execute_reply":"2023-07-04T17:54:09.930089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sentence Completion using a Text generation transformer model","metadata":{}},{"cell_type":"markdown","source":"The second section of the problem will be tackled first to build a huggingfaces pipeline that is capable of generating a short story script when provided with one or more sentence starters","metadata":{}},{"cell_type":"markdown","source":"1. The bookcorpus dataset available on the huggingface model hub can be used to fine tune a GPT-based text generation model for making it suitable to the task of story script generation. However, the dataset is too large in a resource crunched environment to train within a reasonable amount of time. So for the purpose of this modeling exercise, around 60,000 samples are extracted from the entire dataset and used for fine tuning purpose. The following code which is commented out serves to extract the 60,000 random text samples. This could be uncommented and run only once when the notebook is executed for the first time. After the first run, the condensed dataset of 60,000 samples will be stored due to which we can again comment out the code section for the subsequent runs. In this step, the training dataset of 60,000 samples can be further split into training and validation sets by selecting a suitable split ratio","metadata":{}},{"cell_type":"code","source":"#from datasets import load_dataset\n#import datasets\n#raw_story_scripts_train = load_dataset('bookcorpus',split='train[:20%]+train[-20%:]')\n#condensed_size = 60000\n#raw_story_scripts={'train':raw_story_scripts_train}\n#raw_story_scripts = datasets.DatasetDict(raw_story_scripts)\n#raw_story_scripts_condensed = raw_story_scripts['train'].shuffle(seed=42).select(range(condensed_size))\n#split_dataset = raw_story_scripts_condensed.train_test_split(test_size=0.2,seed=42)\n#split_dataset['validation'] = split_dataset.pop('test')\n#for split,dataset in split_dataset.items():\n#    dataset.to_json(f'tiny-book-corpus-{split}.jsonl')","metadata":{"execution":{"iopub.status.busy":"2023-07-04T10:52:20.998548Z","iopub.execute_input":"2023-07-04T10:52:20.999982Z","iopub.status.idle":"2023-07-04T11:18:41.349512Z","shell.execute_reply.started":"2023-07-04T10:52:20.999927Z","shell.execute_reply":"2023-07-04T11:18:41.347316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. The stored 60,000 samples can be loaded by providing the path of the local directory where the samples are stored. It should be ensured that the path string **/kaggle/input/inputs/** is replaced by the correct local directory path","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\ndata_files = {'train':'/kaggle/input/inputs/tiny-book-corpus-train.jsonl','test':'/kaggle/input/inputs/tiny-book-corpus-validation.jsonl'}\nsplit_dataset = load_dataset('json',data_files=data_files)\nsplit_dataset['validation'] = split_dataset.pop('test')","metadata":{"execution":{"iopub.status.busy":"2023-07-04T18:12:01.291105Z","iopub.execute_input":"2023-07-04T18:12:01.291511Z","iopub.status.idle":"2023-07-04T18:12:01.872702Z","shell.execute_reply.started":"2023-07-04T18:12:01.291476Z","shell.execute_reply":"2023-07-04T18:12:01.871722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"split_dataset","metadata":{"execution":{"iopub.status.busy":"2023-07-04T18:12:07.376069Z","iopub.execute_input":"2023-07-04T18:12:07.376464Z","iopub.status.idle":"2023-07-04T18:12:07.382607Z","shell.execute_reply.started":"2023-07-04T18:12:07.376431Z","shell.execute_reply":"2023-07-04T18:12:07.381592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3. The training corpus dataset is sequentially subjected to tokenization and encoding following which it is used to fine-tune (just train the model head) of a text generation model. The model used here is a gpt2 like model with the checkpoint of 'distilgpt2'. This checkpoint is chosen because the underlying model is a lightweight version of the larger gpt2 model. The huggingface model hub provides the flexibility to load pretrained transformer models along with the tokenizers that are compatible with the particular model","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, GPT2LMHeadModel\nmodel_checkpoint = 'distilgpt2'\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\nmodel = GPT2LMHeadModel.from_pretrained(model_checkpoint)","metadata":{"execution":{"iopub.status.busy":"2023-07-04T18:12:12.565571Z","iopub.execute_input":"2023-07-04T18:12:12.565930Z","iopub.status.idle":"2023-07-04T18:12:17.875601Z","shell.execute_reply.started":"2023-07-04T18:12:12.565901Z","shell.execute_reply":"2023-07-04T18:12:17.874558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4. Training the model head on short sentences is not likely to be an optimal way of learning the story generation approach. Hence, the next step is to identify the word length corresponding to each text input and filtering out sentences that are less than 15 words in length","metadata":{}},{"cell_type":"code","source":"def count_str_length(el):\n    return {'word_length':[len(sample.split(' ')) for sample in el['text']]}\n\n\nsplit_dataset = split_dataset.map(count_str_length,batched=True)\nsplit_dataset.set_format('pandas')\nsplit_dataset.reset_format()\nsplit_dataset = split_dataset.filter(lambda x: x['word_length']>=15)\nsplit_dataset","metadata":{"execution":{"iopub.status.busy":"2023-07-04T17:56:27.535118Z","iopub.execute_input":"2023-07-04T17:56:27.535489Z","iopub.status.idle":"2023-07-04T17:56:27.540934Z","shell.execute_reply.started":"2023-07-04T17:56:27.535458Z","shell.execute_reply":"2023-07-04T17:56:27.539565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"5. The input strings are tokenized and encoded next through the tokenizer object to be stored with key of 'input_ids' in the training data set dictionary post execution of this step. It shuould be noted here that the tokens are truncated to a length of 512 and the truncated tokens are also populated as the subsequent entry in the encoded output","metadata":{}},{"cell_type":"code","source":"context_length=512\ndef tokenize_scripts(el):\n    tok_output = tokenizer(el['text'], truncation=True, max_length=context_length,\n                    return_overflowing_tokens=True,return_length=True)\n    return {'input_ids':tok_output['input_ids']}\n\ntokenized_dataset = split_dataset.map(tokenize_scripts,batched=True,\n                                      remove_columns=split_dataset['train'].column_names)\ntokenized_dataset","metadata":{"execution":{"iopub.status.busy":"2023-07-04T18:12:52.813808Z","iopub.execute_input":"2023-07-04T18:12:52.814159Z","iopub.status.idle":"2023-07-04T18:12:52.820921Z","shell.execute_reply.started":"2023-07-04T18:12:52.814130Z","shell.execute_reply":"2023-07-04T18:12:52.818576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"6. Next a data collator with the DataCollatorForLanguageModeling class is instantiated to translate the encoded tokens into batches for fine-tuning the model (training the model head)","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling\ntokenizer.pad_token = tokenizer.eos_token\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,mlm=False)\nout = data_collator([tokenized_dataset['train'][i] for i in range(3)])\nfor key in out:\n    print(f'{key} shape:{out[key].shape}')","metadata":{"execution":{"iopub.status.busy":"2023-07-04T18:13:21.912276Z","iopub.execute_input":"2023-07-04T18:13:21.912658Z","iopub.status.idle":"2023-07-04T18:13:22.459568Z","shell.execute_reply.started":"2023-07-04T18:13:21.912629Z","shell.execute_reply":"2023-07-04T18:13:22.458409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2023-07-04T18:14:04.047054Z","iopub.execute_input":"2023-07-04T18:14:04.047474Z","iopub.status.idle":"2023-07-04T18:14:04.097285Z","shell.execute_reply.started":"2023-07-04T18:14:04.047440Z","shell.execute_reply":"2023-07-04T18:14:04.096399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"7. The hugging face libraries have wrappers to abstract the process of learner instantiation and configuration with the classes named TrainingArguments and Trainer. These classes can be used to set thee model hyperparameters like number of epochs, leanrning rate, weight decay along with identification of the encoded training and validation tokens. The fine-tuned model is stored with the name of 'tiny-random-GPT2LMHeadModel-finetuned-corpus' on the hugging face model hub so that it can be invoked through a pipeline later on and used for the story script generation","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\nargs = TrainingArguments(\n    f'tiny-random-GPT2LMHeadModel-finetuned-corpus',\n    overwrite_output_dir=True,\n    num_train_epochs=3,\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    learning_rate=5e-4,\n    weight_decay = 0.01,\n    fp16=True,\n    push_to_hub=True\n)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset = tokenized_dataset['train'],\n    eval_dataset = tokenized_dataset['validation'],\n    tokenizer = tokenizer,\n    data_collator = data_collator\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-04T18:14:35.398924Z","iopub.execute_input":"2023-07-04T18:14:35.399300Z","iopub.status.idle":"2023-07-04T18:16:35.540675Z","shell.execute_reply.started":"2023-07-04T18:14:35.399268Z","shell.execute_reply":"2023-07-04T18:16:35.539584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"8. The train() method of the Trainer object can be used to commence the fine-tuning process which will take some time post which the model will be pushed to the hugging face model hub","metadata":{}},{"cell_type":"code","source":"trainer.train()\ntrainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2023-07-04T18:16:41.446020Z","iopub.execute_input":"2023-07-04T18:16:41.446436Z","iopub.status.idle":"2023-07-04T18:29:43.457300Z","shell.execute_reply.started":"2023-07-04T18:16:41.446404Z","shell.execute_reply":"2023-07-04T18:29:43.456233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from transformers import pipeline\n#story_teller = pipeline('text-generation',model='tiny-random-GPT2LMHeadModel-finetuned-corpus')\n#story_teller('Once upon')","metadata":{"execution":{"iopub.status.busy":"2023-07-04T13:00:08.464788Z","iopub.execute_input":"2023-07-04T13:00:08.465191Z","iopub.status.idle":"2023-07-04T13:00:14.058854Z","shell.execute_reply.started":"2023-07-04T13:00:08.465161Z","shell.execute_reply":"2023-07-04T13:00:14.057835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#story_teller('You are')[0]['generated_text']","metadata":{"execution":{"iopub.status.busy":"2023-07-04T13:01:15.946319Z","iopub.execute_input":"2023-07-04T13:01:15.946687Z","iopub.status.idle":"2023-07-04T13:01:17.566728Z","shell.execute_reply.started":"2023-07-04T13:01:15.946657Z","shell.execute_reply":"2023-07-04T13:01:17.560805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#story_teller('Roses are red')[0]['generated_text']","metadata":{"execution":{"iopub.status.busy":"2023-07-04T13:07:21.535054Z","iopub.execute_input":"2023-07-04T13:07:21.536088Z","iopub.status.idle":"2023-07-04T13:07:23.082149Z","shell.execute_reply.started":"2023-07-04T13:07:21.536032Z","shell.execute_reply":"2023-07-04T13:07:23.081127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#normal_generator = pipeline('text-generation')\n#normal_generator('Roses are red')[0]['generated_text']","metadata":{"execution":{"iopub.status.busy":"2023-07-04T13:08:33.188693Z","iopub.execute_input":"2023-07-04T13:08:33.189809Z","iopub.status.idle":"2023-07-04T13:08:43.562409Z","shell.execute_reply.started":"2023-07-04T13:08:33.189767Z","shell.execute_reply":"2023-07-04T13:08:43.560335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#normal_generator('Once upon')[0]['generated_text']","metadata":{"execution":{"iopub.status.busy":"2023-07-04T13:09:13.329164Z","iopub.execute_input":"2023-07-04T13:09:13.330170Z","iopub.status.idle":"2023-07-04T13:09:15.863495Z","shell.execute_reply.started":"2023-07-04T13:09:13.330126Z","shell.execute_reply":"2023-07-04T13:09:15.859233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#story_teller('My love')[0]['generated_text']","metadata":{"execution":{"iopub.status.busy":"2023-07-04T13:09:43.303829Z","iopub.execute_input":"2023-07-04T13:09:43.304233Z","iopub.status.idle":"2023-07-04T13:09:44.929463Z","shell.execute_reply.started":"2023-07-04T13:09:43.304200Z","shell.execute_reply":"2023-07-04T13:09:44.925375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Semantic search for seeding phrases through a Sentence Classification Transformer model","metadata":{}},{"cell_type":"markdown","source":"Now that the story content generation model is ready, it is time to build the other model that can return a philosophical quote generating the best match with an input word representing a story genre. The matching logic is implemented with the help of semantic search which returns the quotes whose token embeddings generate the best cosine scores with the embedding of the input story genre","metadata":{}},{"cell_type":"markdown","source":"1. For achieving the semantic search logic, the corpus of philosophical quotes available on the hugging face model hub is leveraged. The training set is limited to quotes with word length in between 5 and 50 because the subsequent text generation model should get sufficient leeway to generate script words for the story to be meaningful","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nraw_quote_dataset = load_dataset('mertbozkurt/quotes_philosophers',encoding=\"latin-1\")\nraw_quote_dataset = raw_quote_dataset.map(count_str_length,batched=True)\nraw_quote_dataset = raw_quote_dataset.filter(lambda x: (x['word_length']<=50)&(x['word_length']>=5))\nraw_quote_dataset","metadata":{"execution":{"iopub.status.busy":"2023-07-04T17:56:02.791673Z","iopub.execute_input":"2023-07-04T17:56:02.792055Z","iopub.status.idle":"2023-07-04T17:56:07.355997Z","shell.execute_reply.started":"2023-07-04T17:56:02.792021Z","shell.execute_reply":"2023-07-04T17:56:07.355082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#len_dist = raw_quote_dataset['train']['word_length'].value_counts().to_frame().reset_index().rename(columns={'index':'word_length','word_length':'len_count'}).sort_values('word_length',ascending=False)\n#len_dist","metadata":{"execution":{"iopub.status.busy":"2023-07-04T17:56:38.985412Z","iopub.execute_input":"2023-07-04T17:56:38.985764Z","iopub.status.idle":"2023-07-04T17:56:39.036077Z","shell.execute_reply.started":"2023-07-04T17:56:38.985735Z","shell.execute_reply":"2023-07-04T17:56:39.034874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sum(len_dist['len_count'])","metadata":{"execution":{"iopub.status.busy":"2023-07-04T17:23:07.331303Z","iopub.execute_input":"2023-07-04T17:23:07.331685Z","iopub.status.idle":"2023-07-04T17:23:07.337299Z","shell.execute_reply.started":"2023-07-04T17:23:07.331655Z","shell.execute_reply":"2023-07-04T17:23:07.336588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sum(len_dist[(len_dist.word_length<50) & (len_dist.word_length>=5)]['len_count'])","metadata":{"execution":{"iopub.status.busy":"2023-07-04T17:56:46.892546Z","iopub.execute_input":"2023-07-04T17:56:46.893126Z","iopub.status.idle":"2023-07-04T17:56:46.901663Z","shell.execute_reply.started":"2023-07-04T17:56:46.893089Z","shell.execute_reply":"2023-07-04T17:56:46.900483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#raw_quote_dataset.reset_format()\n#raw_quote_dataset = raw_quote_dataset.filter(lambda x: (x['word_length']<=50)&(x['word_length']>=5))\n#raw_quote_dataset","metadata":{"execution":{"iopub.status.busy":"2023-07-04T17:56:51.995589Z","iopub.execute_input":"2023-07-04T17:56:51.995967Z","iopub.status.idle":"2023-07-04T17:56:52.054465Z","shell.execute_reply.started":"2023-07-04T17:56:51.995925Z","shell.execute_reply":"2023-07-04T17:56:52.053537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. The tokenizer and architecture for the sentence classification model required here correspond to the checkpoint of 'multi-qa-mpnet-base-dot-v1'. The model and the compatible tokenizer are loaded with the suitable hugging face routines","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\nimport torch\nss_checkpoint = 'sentence-transformers/multi-qa-mpnet-base-dot-v1'\nss_tokenizer = AutoTokenizer.from_pretrained(ss_checkpoint)\nss_model = AutoModel.from_pretrained(ss_checkpoint)\n\ndevice = torch.device('cuda') if torch.device('cuda') is not None else torch.device('cpu')\nss_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-07-04T17:56:56.614686Z","iopub.execute_input":"2023-07-04T17:56:56.615034Z","iopub.status.idle":"2023-07-04T17:57:34.733632Z","shell.execute_reply.started":"2023-07-04T17:56:56.615007Z","shell.execute_reply":"2023-07-04T17:57:34.732385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3. Semantic search works by generating embedding vectors for the search string as well as the philosophical quote corpus and then computing the dot product similarity between the embedding vectors to find the closest matching quote from the corpus. The method that follows generates an embedding vector through a method known as cls pooling by taking text phrase as input","metadata":{}},{"cell_type":"code","source":"def cls_pooling(model_out):\n    return model_out.last_hidden_state[:,0]\n\ndef generate_embeddings(el):\n    tkzd_inputs = ss_tokenizer(el,padding=True,return_tensors='pt')\n    tkzd_inputs = {k:v.to(device) for k,v in tkzd_inputs.items()}\n    outputs = ss_model(**tkzd_inputs)\n    return cls_pooling(outputs)","metadata":{"execution":{"iopub.status.busy":"2023-07-04T18:09:38.531913Z","iopub.execute_input":"2023-07-04T18:09:38.532299Z","iopub.status.idle":"2023-07-04T18:09:38.540748Z","shell.execute_reply.started":"2023-07-04T18:09:38.532268Z","shell.execute_reply":"2023-07-04T18:09:38.539588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4. The corpus of quotes is then subjected to the generate_embeddings() function to create an embedding vector for each quote in the corpus. For computing dot product similarity on the embedding vectors, the faiss (Facebook AI Similarity Search) algorithm is used ","metadata":{}},{"cell_type":"code","source":"raw_quote_dataset = raw_quote_dataset['train']\nembedding_space = raw_quote_dataset.map(lambda x:{'embedding':generate_embeddings(x['text']).detach().cpu().numpy()[0]})\nembedding_space.add_faiss_index(column='embedding')","metadata":{"execution":{"iopub.status.busy":"2023-07-04T18:09:45.216038Z","iopub.execute_input":"2023-07-04T18:09:45.216404Z","iopub.status.idle":"2023-07-04T18:10:18.213438Z","shell.execute_reply.started":"2023-07-04T18:09:45.216374Z","shell.execute_reply":"2023-07-04T18:10:18.212252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"5. The search string is also then translated into an embedding vector and then compared with the embedding vectors of the corpus through the dot product similarity metric by leveraging the faiss technique. The semantic_search() function returns the top 5 matching quotes from the corpus corresponding to the search string that is passed as argument","metadata":{}},{"cell_type":"code","source":"def semantic_search(question):\n    question_embedding = generate_embeddings([question]).detach().cpu().numpy()[0]\n    scores,samples = embedding_space.get_nearest_examples('embedding',question_embedding,k=5)\n    sample_df = pd.DataFrame.from_dict(samples)\n    sample_df['scores'] = scores\n    sample_df.sort_values('scores',ascending=False,inplace=True)\n    return sample_df","metadata":{"execution":{"iopub.status.busy":"2023-07-04T18:11:12.592123Z","iopub.execute_input":"2023-07-04T18:11:12.592509Z","iopub.status.idle":"2023-07-04T18:11:12.598928Z","shell.execute_reply.started":"2023-07-04T18:11:12.592480Z","shell.execute_reply":"2023-07-04T18:11:12.597901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"story_starters = []\nsearch_results = semantic_search('what is life?')\nfor _,row in search_results.iterrows():\n    story_starters.append(row.text)\nstory_starters","metadata":{"execution":{"iopub.status.busy":"2023-07-04T19:15:06.706908Z","iopub.execute_input":"2023-07-04T19:15:06.707297Z","iopub.status.idle":"2023-07-04T19:15:06.749442Z","shell.execute_reply.started":"2023-07-04T19:15:06.707267Z","shell.execute_reply":"2023-07-04T19:15:06.748379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\nstory_teller = pipeline('text-generation',model='tiny-random-GPT2LMHeadModel-finetuned-corpus')\nstory_teller(story_starters)","metadata":{"execution":{"iopub.status.busy":"2023-07-04T19:15:26.489273Z","iopub.execute_input":"2023-07-04T19:15:26.490245Z","iopub.status.idle":"2023-07-04T19:15:33.432829Z","shell.execute_reply.started":"2023-07-04T19:15:26.490210Z","shell.execute_reply":"2023-07-04T19:15:33.431696Z"},"trusted":true},"execution_count":null,"outputs":[]}]}