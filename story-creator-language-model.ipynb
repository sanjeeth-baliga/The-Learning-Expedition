{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"The code that follows leverages language transformer models to perform a sequence of two tasks that enables the generation of a short story script in the english language\n1. **Semantic search** through a transformer fine-tuned to the sentence classification problem to extract the most relevant philosophical quote macthing an input key word from a larger corpus\n2. **Text generation** by fine tuning a transformer model to the sentence completion problem by leveraging the words generated from the earlier stage to start a new sentence\n\nThe huggingface model hub has been used to load the required datasets and fine-tune the transformer models for accomplishing the task described above. I would advise you to take a course offered by huggingfaces linked [here](https://huggingface.co/learn/nlp-course/chapter0/1?fw=pt) if you are interested in exploring this further or checking out other language models","metadata":{}},{"cell_type":"markdown","source":"This code has been built and tested in the kaggle ecosystem. Hence, if a similar notebook environment is utilized by a user, it is advisable to make all installations associated with libraries involving the huggingface models","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n!pip install datasets\n!pip install -U git+https://github.com/huggingface/transformers.git\n!pip install -U git+https://github.com/huggingface/accelerate.git\n!pip install faiss-cpu","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-15T14:17:09.822781Z","iopub.execute_input":"2023-07-15T14:17:09.823089Z","iopub.status.idle":"2023-07-15T14:18:46.666651Z","shell.execute_reply.started":"2023-07-15T14:17:09.823061Z","shell.execute_reply":"2023-07-15T14:18:46.665380Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/inputs/tiny-book-corpus-validation.jsonl\n/kaggle/input/inputs/tiny-book-corpus-train.jsonl\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.23.5)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (1.5.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.28.2)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.64.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.15.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.12.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (5.4.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.5.7)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting git+https://github.com/huggingface/transformers.git\n  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-4hldwzfh\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-4hldwzfh\n  Resolved https://github.com/huggingface/transformers.git to commit 5bb4430edc7df9f9950d412d98bbe505cc4d328b\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (3.12.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (0.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (5.4.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (2023.5.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (2.28.2)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (0.3.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0.dev0) (4.64.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0.dev0) (2023.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0.dev0) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.31.0.dev0) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0.dev0) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0.dev0) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0.dev0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0.dev0) (2023.5.7)\nBuilding wheels for collected packages: transformers\n  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for transformers: filename=transformers-4.31.0.dev0-py3-none-any.whl size=7352645 sha256=4afb0f29e60526dae4e3fc5170c1504d7fa02360e2f7121524203019fb343f43\n  Stored in directory: /tmp/pip-ephem-wheel-cache-0pdi4g32/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\nSuccessfully built transformers\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.30.1\n    Uninstalling transformers-4.30.1:\n      Successfully uninstalled transformers-4.30.1\nSuccessfully installed transformers-4.31.0.dev0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting git+https://github.com/huggingface/accelerate.git\n  Cloning https://github.com/huggingface/accelerate.git to /tmp/pip-req-build-c0ki1zrb\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate.git /tmp/pip-req-build-c0ki1zrb\n  Resolved https://github.com/huggingface/accelerate.git to commit 299f3ef8ab40f764f3cde8e854f3fe9d3c047f73\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0.dev0) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0.dev0) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0.dev0) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0.dev0) (5.4.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0.dev0) (2.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate==0.22.0.dev0) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0.dev0) (3.12.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0.dev0) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0.dev0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0.dev0) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0.dev0) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.22.0.dev0) (2.1.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.22.0.dev0) (1.3.0)\nBuilding wheels for collected packages: accelerate\n  Building wheel for accelerate (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for accelerate: filename=accelerate-0.22.0.dev0-py3-none-any.whl size=245128 sha256=ab81b0029e826e30700985c617135ecba7f618a8b3d976acc0e69228a189a2d1\n  Stored in directory: /tmp/pip-ephem-wheel-cache-wcdl9fzy/wheels/9c/a3/1e/47368f9b6575655fe9ee1b6350cfa7d4b0befe66a35f8a8365\nSuccessfully built accelerate\nInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.12.0\n    Uninstalling accelerate-0.12.0:\n      Successfully uninstalled accelerate-0.12.0\nSuccessfully installed accelerate-0.22.0.dev0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting faiss-cpu\n  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-cpu\nSuccessfully installed faiss-cpu-1.7.4\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"# Sentence Completion using a Text generation transformer model","metadata":{}},{"cell_type":"markdown","source":"The second section of the problem will be tackled first to build a huggingfaces pipeline that is capable of generating a short story script when provided with one or more sentence starters","metadata":{}},{"cell_type":"markdown","source":"1. The bookcorpus dataset available on the huggingface model hub can be used to fine tune a GPT-based text generation model for making it suitable to the task of story script generation. However, the dataset is too large in a resource crunched environment to train within a reasonable amount of time. So for the purpose of this modeling exercise, around 60,000 samples are extracted from the entire dataset and used for fine tuning purpose. The following code which is commented out serves to extract the 60,000 random text samples. This could be uncommented and run only once when the notebook is executed for the first time. After the first run, the condensed dataset of 60,000 samples will be stored due to which we can again comment out the code section for the subsequent runs. In this step, the training dataset of 60,000 samples can be further split into training and validation sets by selecting a suitable split ratio","metadata":{}},{"cell_type":"code","source":"#from datasets import load_dataset\n#import datasets\n#raw_story_scripts_train = load_dataset('bookcorpus',split='train[:20%]+train[-20%:]')\n#condensed_size = 60000\n#raw_story_scripts={'train':raw_story_scripts_train}\n#raw_story_scripts = datasets.DatasetDict(raw_story_scripts)\n#raw_story_scripts_condensed = raw_story_scripts['train'].shuffle(seed=42).select(range(condensed_size))\n#split_dataset = raw_story_scripts_condensed.train_test_split(test_size=0.2,seed=42)\n#split_dataset['validation'] = split_dataset.pop('test')\n#for split,dataset in split_dataset.items():\n#    dataset.to_json(f'tiny-book-corpus-{split}.jsonl')","metadata":{"execution":{"iopub.status.busy":"2023-07-15T14:18:46.670446Z","iopub.execute_input":"2023-07-15T14:18:46.670784Z","iopub.status.idle":"2023-07-15T14:18:46.676650Z","shell.execute_reply.started":"2023-07-15T14:18:46.670755Z","shell.execute_reply":"2023-07-15T14:18:46.675490Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"2. The stored 60,000 samples can be loaded by providing the path of the local directory where the samples are stored. It should be ensured that the path string **/kaggle/input/inputs/** is replaced by the correct local directory path","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\ndata_files = {'train':'/kaggle/input/inputs/tiny-book-corpus-train.jsonl','test':'/kaggle/input/inputs/tiny-book-corpus-validation.jsonl'}\nsplit_dataset = load_dataset('json',data_files=data_files)\nsplit_dataset['validation'] = split_dataset.pop('test')","metadata":{"execution":{"iopub.status.busy":"2023-07-15T14:18:46.678090Z","iopub.execute_input":"2023-07-15T14:18:46.679214Z","iopub.status.idle":"2023-07-15T14:18:48.066257Z","shell.execute_reply.started":"2023-07-15T14:18:46.679183Z","shell.execute_reply":"2023-07-15T14:18:48.065359Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-f976537655f3a925/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3deefde19d064ec99d7e372644eb16f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e50aca356c94b4e8a0bb36acdfd17fd"}},"metadata":{}},{"name":"stdout","text":"Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-f976537655f3a925/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ff9192b46ec4da7b502b51305a5b697"}},"metadata":{}}]},{"cell_type":"code","source":"split_dataset","metadata":{"execution":{"iopub.status.busy":"2023-07-15T14:18:48.068835Z","iopub.execute_input":"2023-07-15T14:18:48.069978Z","iopub.status.idle":"2023-07-15T14:18:48.077692Z","shell.execute_reply.started":"2023-07-15T14:18:48.069939Z","shell.execute_reply":"2023-07-15T14:18:48.076730Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 48000\n    })\n    validation: Dataset({\n        features: ['text'],\n        num_rows: 12000\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"3. The training corpus dataset is sequentially subjected to tokenization and encoding following which it is used to fine-tune (just train the model head) of a text generation model. The model used here is a gpt2 like model with the checkpoint of 'distilgpt2'. This checkpoint is chosen because the underlying model is a lightweight version of the larger gpt2 model. The huggingface model hub provides the flexibility to load pretrained transformer models along with the tokenizers that are compatible with the particular model","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, GPT2LMHeadModel\nmodel_checkpoint = 'distilgpt2'\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\nmodel = GPT2LMHeadModel.from_pretrained(model_checkpoint)","metadata":{"execution":{"iopub.status.busy":"2023-07-15T14:18:48.079437Z","iopub.execute_input":"2023-07-15T14:18:48.080143Z","iopub.status.idle":"2023-07-15T14:19:07.521009Z","shell.execute_reply.started":"2023-07-15T14:18:48.080091Z","shell.execute_reply":"2023-07-15T14:19:07.520086Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5518e1cf664a4bff87293d0c1b480590"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ad6b9ed41134bb691c2836993c08dda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a35a2322e9bb45ee8ac231d76401e5a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f8d83b2b5594d2286baf1bc7d45eb76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7896ff530b2946e79c5b91f2d02ec10d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86415a4819424b5ca5f8bd8fb39aac2b"}},"metadata":{}}]},{"cell_type":"markdown","source":"4. Training the model head on short sentences is not likely to be an optimal way of learning the story generation approach. Hence, the next step is to identify the word length corresponding to each text input and filtering out sentences that are less than 15 words in length","metadata":{}},{"cell_type":"code","source":"def count_str_length(el):\n    return {'word_length':[len(sample.split(' ')) for sample in el['text']]}\n\n\nsplit_dataset = split_dataset.map(count_str_length,batched=True)\nsplit_dataset.set_format('pandas')\nsplit_dataset.reset_format()\nsplit_dataset = split_dataset.filter(lambda x: x['word_length']>=15)\nsplit_dataset","metadata":{"execution":{"iopub.status.busy":"2023-07-15T14:19:07.522701Z","iopub.execute_input":"2023-07-15T14:19:07.525626Z","iopub.status.idle":"2023-07-15T14:19:08.235823Z","shell.execute_reply.started":"2023-07-15T14:19:07.525589Z","shell.execute_reply":"2023-07-15T14:19:08.234920Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/48 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd05f12127db4c37ba382c5f22bc5916"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/12 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6331a2e321e94f0385c62fb49d9293d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/48 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"572e6222f592465ba521e810b6fb6b6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/12 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"902c66aebd704234b5aedbca39898ccf"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'word_length'],\n        num_rows: 16994\n    })\n    validation: Dataset({\n        features: ['text', 'word_length'],\n        num_rows: 4275\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"5. The input strings are tokenized and encoded next through the tokenizer object to be stored with key of 'input_ids' in the training data set dictionary post execution of this step. It shuould be noted here that the tokens are truncated to a length of 512 and the truncated tokens are also populated as the subsequent entry in the encoded output","metadata":{}},{"cell_type":"code","source":"context_length=512\ndef tokenize_scripts(el):\n    tok_output = tokenizer(el['text'], truncation=True, max_length=context_length,\n                    return_overflowing_tokens=True,return_length=True)\n    return {'input_ids':tok_output['input_ids']}\n\ntokenized_dataset = split_dataset.map(tokenize_scripts,batched=True,\n                                      remove_columns=split_dataset['train'].column_names)\ntokenized_dataset","metadata":{"execution":{"iopub.status.busy":"2023-07-15T14:19:08.237417Z","iopub.execute_input":"2023-07-15T14:19:08.238340Z","iopub.status.idle":"2023-07-15T14:19:10.091274Z","shell.execute_reply.started":"2023-07-15T14:19:08.238290Z","shell.execute_reply":"2023-07-15T14:19:10.090382Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/17 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2aaf06db4682458c98bdb1d0c5c9ee2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2865e31037144e9fb16028a11b23d440"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids'],\n        num_rows: 16994\n    })\n    validation: Dataset({\n        features: ['input_ids'],\n        num_rows: 4275\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"6. Next a data collator with the DataCollatorForLanguageModeling class is instantiated to translate the encoded tokens into batches for fine-tuning the model (training the model head)","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling\ntokenizer.pad_token = tokenizer.eos_token\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,mlm=False)\nout = data_collator([tokenized_dataset['train'][i] for i in range(3)])\nfor key in out:\n    print(f'{key} shape:{out[key].shape}')","metadata":{"execution":{"iopub.status.busy":"2023-07-15T14:19:10.093167Z","iopub.execute_input":"2023-07-15T14:19:10.093843Z","iopub.status.idle":"2023-07-15T14:19:10.518849Z","shell.execute_reply.started":"2023-07-15T14:19:10.093798Z","shell.execute_reply":"2023-07-15T14:19:10.517783Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"name":"stdout","text":"input_ids shape:torch.Size([3, 46])\nattention_mask shape:torch.Size([3, 46])\nlabels shape:torch.Size([3, 46])\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2023-07-15T14:19:10.520153Z","iopub.execute_input":"2023-07-15T14:19:10.520520Z","iopub.status.idle":"2023-07-15T14:19:10.550300Z","shell.execute_reply.started":"2023-07-15T14:19:10.520486Z","shell.execute_reply":"2023-07-15T14:19:10.549262Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"979edfe597e6480bb6d0420eb3323e4c"}},"metadata":{}}]},{"cell_type":"markdown","source":"7. The hugging face libraries have wrappers to abstract the process of learner instantiation and configuration with the classes named TrainingArguments and Trainer. These classes can be used to set thee model hyperparameters like number of epochs, leanrning rate, weight decay along with identification of the encoded training and validation tokens. The fine-tuned model is stored with the name of 'tiny-random-GPT2LMHeadModel-finetuned-corpus' on the hugging face model hub so that it can be invoked through a pipeline later on and used for the story script generation","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\nargs = TrainingArguments(\n    f'tiny-random-GPT2LMHeadModel-finetuned-corpus',\n    overwrite_output_dir=True,\n    num_train_epochs=3,\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    learning_rate=5e-4,\n    weight_decay = 0.01,\n    fp16=True,\n    push_to_hub=True\n)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset = tokenized_dataset['train'],\n    eval_dataset = tokenized_dataset['validation'],\n    tokenizer = tokenizer,\n    data_collator = data_collator\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-15T14:20:23.666676Z","iopub.execute_input":"2023-07-15T14:20:23.667117Z","iopub.status.idle":"2023-07-15T14:22:00.679859Z","shell.execute_reply.started":"2023-07-15T14:20:23.667081Z","shell.execute_reply":"2023-07-15T14:22:00.678454Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Cloning https://huggingface.co/san94/tiny-random-GPT2LMHeadModel-finetuned-corpus into local empty directory.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Download file pytorch_model.bin:   0%|          | 8.00k/312M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f19512d6019d4417bfc1442576724f1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Download file runs/Jul04_12-41-46_20b74ae67198/events.out.tfevents.1688474526.20b74ae67198.28.0: 100%|########…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa79a4925e3c4872b6e116a98b0e2fe1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Download file runs/Jul04_18-14-35_8ce4b887e41a/events.out.tfevents.1688494601.8ce4b887e41a.28.0: 100%|########…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5fcd35119e04f47a61b1d438a27bb62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Download file training_args.bin: 100%|##########| 3.93k/3.93k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6c34ac1af554427859a58d66ff0b132"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Clean file runs/Jul04_12-41-46_20b74ae67198/events.out.tfevents.1688474526.20b74ae67198.28.0:  16%|#5        |…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fc7be5dca864332af1ae0e5003cdcbe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Clean file runs/Jul04_18-14-35_8ce4b887e41a/events.out.tfevents.1688494601.8ce4b887e41a.28.0:  16%|#5        |…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20db44c4e5db4ec5a369217cebdf9e32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Clean file training_args.bin:  25%|##5       | 1.00k/3.93k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a18481b47364ae1b368737f84fafc7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Clean file pytorch_model.bin:   0%|          | 1.00k/312M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49423cc6fd5b4ef19bb805a765c95309"}},"metadata":{}}]},{"cell_type":"markdown","source":"8. The train() method of the Trainer object can be used to commence the fine-tuning process which will take some time post which the model will be pushed to the hugging face model hub","metadata":{}},{"cell_type":"code","source":"trainer.train()\ntrainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2023-07-15T14:22:27.903355Z","iopub.execute_input":"2023-07-15T14:22:27.903725Z","iopub.status.idle":"2023-07-15T14:35:06.246011Z","shell.execute_reply.started":"2023-07-15T14:22:27.903696Z","shell.execute_reply":"2023-07-15T14:35:06.245007Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.5 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230715_142239-emyq2sim</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/baligasanjeeth/huggingface/runs/emyq2sim' target=\"_blank\">restful-glade-6</a></strong> to <a href='https://wandb.ai/baligasanjeeth/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/baligasanjeeth/huggingface' target=\"_blank\">https://wandb.ai/baligasanjeeth/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/baligasanjeeth/huggingface/runs/emyq2sim' target=\"_blank\">https://wandb.ai/baligasanjeeth/huggingface/runs/emyq2sim</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3189' max='3189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3189/3189 11:33, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>4.443300</td>\n      <td>4.278949</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3.701300</td>\n      <td>4.251234</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>3.041200</td>\n      <td>4.449711</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSeveral commits (2) will be pushed upstream.\nThe progress bars may be unreliable.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Upload file runs/Jul15_14-20-23_bf33ad69f349/events.out.tfevents.1689430948.bf33ad69f349.29.0:   0%|          …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a165cc1e93dc41e3be11a54fe0ffc780"}},"metadata":{}},{"name":"stderr","text":"To https://huggingface.co/san94/tiny-random-GPT2LMHeadModel-finetuned-corpus\n   90f81cf..e4d7c0f  main -> main\n\nTo https://huggingface.co/san94/tiny-random-GPT2LMHeadModel-finetuned-corpus\n   e4d7c0f..bcf9fb1  main -> main\n\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'https://huggingface.co/san94/tiny-random-GPT2LMHeadModel-finetuned-corpus/commit/e4d7c0ff9284c5fbe28fbf1c4b392a02e7acb81a'"},"metadata":{}}]},{"cell_type":"code","source":"#from transformers import pipeline\n#story_teller = pipeline('text-generation',model='tiny-random-GPT2LMHeadModel-finetuned-corpus')\n#story_teller('Once upon')","metadata":{"execution":{"iopub.status.busy":"2023-07-15T14:19:18.060653Z","iopub.status.idle":"2023-07-15T14:19:18.061117Z","shell.execute_reply.started":"2023-07-15T14:19:18.060882Z","shell.execute_reply":"2023-07-15T14:19:18.060904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#story_teller('You are')[0]['generated_text']","metadata":{"execution":{"iopub.status.busy":"2023-07-15T14:19:18.063139Z","iopub.status.idle":"2023-07-15T14:19:18.064192Z","shell.execute_reply.started":"2023-07-15T14:19:18.063947Z","shell.execute_reply":"2023-07-15T14:19:18.063970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#story_teller('Roses are red')[0]['generated_text']","metadata":{"execution":{"iopub.status.busy":"2023-07-15T14:19:18.065423Z","iopub.status.idle":"2023-07-15T14:19:18.066242Z","shell.execute_reply.started":"2023-07-15T14:19:18.066005Z","shell.execute_reply":"2023-07-15T14:19:18.066027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#normal_generator = pipeline('text-generation')\n#normal_generator('Roses are red')[0]['generated_text']","metadata":{"execution":{"iopub.status.busy":"2023-07-15T14:19:18.067827Z","iopub.status.idle":"2023-07-15T14:19:18.068899Z","shell.execute_reply.started":"2023-07-15T14:19:18.068648Z","shell.execute_reply":"2023-07-15T14:19:18.068671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#normal_generator('Once upon')[0]['generated_text']","metadata":{"execution":{"iopub.status.busy":"2023-07-15T14:19:18.070116Z","iopub.status.idle":"2023-07-15T14:19:18.071039Z","shell.execute_reply.started":"2023-07-15T14:19:18.070786Z","shell.execute_reply":"2023-07-15T14:19:18.070813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#story_teller('My love')[0]['generated_text']","metadata":{"execution":{"iopub.status.busy":"2023-07-15T14:19:18.072502Z","iopub.status.idle":"2023-07-15T14:19:18.072954Z","shell.execute_reply.started":"2023-07-15T14:19:18.072721Z","shell.execute_reply":"2023-07-15T14:19:18.072743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Semantic search for seeding phrases through a Sentence Classification Transformer model","metadata":{}},{"cell_type":"markdown","source":"Now that the story content generation model is ready, it is time to build the other model that can return a philosophical quote generating the best match with an input word representing a story genre. The matching logic is implemented with the help of semantic search which returns the quotes whose token embeddings generate the best cosine scores with the embedding of the input story genre","metadata":{}},{"cell_type":"markdown","source":"1. For achieving the semantic search logic, the corpus of philosophical quotes available on the hugging face model hub is leveraged. The training set is limited to quotes with word length in between 5 and 50 because the subsequent text generation model should get sufficient leeway to generate script words for the story to be meaningful","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nraw_quote_dataset = load_dataset('mertbozkurt/quotes_philosophers',encoding=\"latin-1\")\nraw_quote_dataset = raw_quote_dataset.map(count_str_length,batched=True)\nraw_quote_dataset = raw_quote_dataset.filter(lambda x: (x['word_length']<=50)&(x['word_length']>=5))\nraw_quote_dataset","metadata":{"execution":{"iopub.status.busy":"2023-07-15T14:39:19.134386Z","iopub.execute_input":"2023-07-15T14:39:19.135990Z","iopub.status.idle":"2023-07-15T14:39:22.471999Z","shell.execute_reply.started":"2023-07-15T14:39:19.135948Z","shell.execute_reply":"2023-07-15T14:39:22.466253Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset text/mertbozkurt--quotes_philosophers to /root/.cache/huggingface/datasets/text/mertbozkurt--quotes_philosophers-da87b9e9f82c0fa9/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c65b3fcc14041c09e76235e1aff095e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/40.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcd250346fb7436190edcba71a9e2cda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/66.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0efeb707899042ccb6f8845af92669f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/30.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1b36db9d6dd4a4f9f3355bb5355d2be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/22.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10b8d6159847437aae92d6bdd96596cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/52.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7a78fa0fba8432b8c47568e0a385013"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/44.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13f114369a564e1eb23b0f09f0db1b9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/7.77k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a37c2ccbfa24065a5d1c750ac884f05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/64.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a773c47ed974792a7964f94f220a02f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/20.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e022d79ffec48a99233c974bf72de86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac562bd455dc48248d9c344743c676da"}},"metadata":{}},{"name":"stdout","text":"Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/mertbozkurt--quotes_philosophers-da87b9e9f82c0fa9/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f708b8003c914f76b14b208e4037f40d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5470a20e6684250989262fbbdabd2f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34f345122a264cb6bb6b371c40102a03"}},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'word_length'],\n        num_rows: 2160\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"#len_dist = raw_quote_dataset['train']['word_length'].value_counts().to_frame().reset_index().rename(columns={'index':'word_length','word_length':'len_count'}).sort_values('word_length',ascending=False)\n#len_dist","metadata":{"execution":{"iopub.status.busy":"2023-07-15T14:19:18.076211Z","iopub.status.idle":"2023-07-15T14:19:18.077398Z","shell.execute_reply.started":"2023-07-15T14:19:18.077122Z","shell.execute_reply":"2023-07-15T14:19:18.077144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sum(len_dist['len_count'])","metadata":{"execution":{"iopub.status.busy":"2023-07-15T14:19:18.078732Z","iopub.status.idle":"2023-07-15T14:19:18.079187Z","shell.execute_reply.started":"2023-07-15T14:19:18.078953Z","shell.execute_reply":"2023-07-15T14:19:18.078975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sum(len_dist[(len_dist.word_length<50) & (len_dist.word_length>=5)]['len_count'])","metadata":{"execution":{"iopub.status.busy":"2023-07-15T14:19:18.080782Z","iopub.status.idle":"2023-07-15T14:19:18.081229Z","shell.execute_reply.started":"2023-07-15T14:19:18.081000Z","shell.execute_reply":"2023-07-15T14:19:18.081020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#raw_quote_dataset.reset_format()\n#raw_quote_dataset = raw_quote_dataset.filter(lambda x: (x['word_length']<=50)&(x['word_length']>=5))\n#raw_quote_dataset","metadata":{"execution":{"iopub.status.busy":"2023-07-15T14:19:18.082840Z","iopub.status.idle":"2023-07-15T14:19:18.083290Z","shell.execute_reply.started":"2023-07-15T14:19:18.083062Z","shell.execute_reply":"2023-07-15T14:19:18.083083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. The tokenizer and architecture for the sentence classification model required here correspond to the checkpoint of 'multi-qa-mpnet-base-dot-v1'. The model and the compatible tokenizer are loaded with the suitable hugging face routines","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\nimport torch\nss_checkpoint = 'sentence-transformers/multi-qa-mpnet-base-dot-v1'\nss_tokenizer = AutoTokenizer.from_pretrained(ss_checkpoint)\nss_model = AutoModel.from_pretrained(ss_checkpoint)\n\ndevice = torch.device('cuda') if torch.device('cuda') is not None else torch.device('cpu')\nss_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-07-15T14:39:39.823187Z","iopub.execute_input":"2023-07-15T14:39:39.823904Z","iopub.status.idle":"2023-07-15T14:39:46.876704Z","shell.execute_reply.started":"2023-07-15T14:39:39.823868Z","shell.execute_reply":"2023-07-15T14:39:46.875379Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6d2459448064cf59931227e13a3dafc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd3368dea4744ad79796eb33210ef374"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"674926304adb4df4b729fc20b67a5532"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e38fa7d059e4f27a3c47f2088024885"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3812e05d116446bbbef3b8392e58017"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"246f826a76f44bb99a724e2f5e95ef56"}},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"MPNetModel(\n  (embeddings): MPNetEmbeddings(\n    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n    (position_embeddings): Embedding(514, 768, padding_idx=1)\n    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): MPNetEncoder(\n    (layer): ModuleList(\n      (0-11): 12 x MPNetLayer(\n        (attention): MPNetAttention(\n          (attn): MPNetSelfAttention(\n            (q): Linear(in_features=768, out_features=768, bias=True)\n            (k): Linear(in_features=768, out_features=768, bias=True)\n            (v): Linear(in_features=768, out_features=768, bias=True)\n            (o): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (intermediate): MPNetIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): MPNetOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (relative_attention_bias): Embedding(32, 12)\n  )\n  (pooler): MPNetPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"3. Semantic search works by generating embedding vectors for the search string as well as the philosophical quote corpus and then computing the dot product similarity between the embedding vectors to find the closest matching quote from the corpus. The method that follows generates an embedding vector through a method known as cls pooling by taking text phrase as input","metadata":{}},{"cell_type":"code","source":"def cls_pooling(model_out):\n    return model_out.last_hidden_state[:,0]\n\ndef generate_embeddings(el):\n    tkzd_inputs = ss_tokenizer(el,padding=True,return_tensors='pt')\n    tkzd_inputs = {k:v.to(device) for k,v in tkzd_inputs.items()}\n    outputs = ss_model(**tkzd_inputs)\n    return cls_pooling(outputs)","metadata":{"execution":{"iopub.status.busy":"2023-07-15T14:43:05.088003Z","iopub.execute_input":"2023-07-15T14:43:05.088440Z","iopub.status.idle":"2023-07-15T14:43:05.098984Z","shell.execute_reply.started":"2023-07-15T14:43:05.088408Z","shell.execute_reply":"2023-07-15T14:43:05.096376Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"4. The corpus of quotes is then subjected to the generate_embeddings() function to create an embedding vector for each quote in the corpus. For computing dot product similarity on the embedding vectors, the faiss (Facebook AI Similarity Search) algorithm is used ","metadata":{}},{"cell_type":"code","source":"raw_quote_dataset = raw_quote_dataset['train']\nembedding_space = raw_quote_dataset.map(lambda x:{'embedding':generate_embeddings(x['text']).detach().cpu().numpy()[0]})\nembedding_space.add_faiss_index(column='embedding')","metadata":{"execution":{"iopub.status.busy":"2023-07-15T14:43:12.895504Z","iopub.execute_input":"2023-07-15T14:43:12.895882Z","iopub.status.idle":"2023-07-15T14:43:46.338815Z","shell.execute_reply.started":"2023-07-15T14:43:12.895852Z","shell.execute_reply":"2023-07-15T14:43:46.337826Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2160 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc40b0877bce47feb08695ec3c2d5e9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddf84144ebd54d7f91949832d14f9768"}},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text', 'word_length', 'embedding'],\n    num_rows: 2160\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"5. The search string is also then translated into an embedding vector and then compared with the embedding vectors of the corpus through the dot product similarity metric by leveraging the faiss technique. The semantic_search() function returns the top 5 semantically similar quotes from the corpus corresponding to the search string that is passed as argument","metadata":{}},{"cell_type":"code","source":"def semantic_search(question):\n    question_embedding = generate_embeddings([question]).detach().cpu().numpy()[0]\n    scores,samples = embedding_space.get_nearest_examples('embedding',question_embedding,k=5)\n    sample_df = pd.DataFrame.from_dict(samples)\n    sample_df['scores'] = scores\n    sample_df.sort_values('scores',ascending=False,inplace=True)\n    return sample_df","metadata":{"execution":{"iopub.status.busy":"2023-07-15T14:59:46.329812Z","iopub.execute_input":"2023-07-15T14:59:46.330266Z","iopub.status.idle":"2023-07-15T14:59:46.339654Z","shell.execute_reply.started":"2023-07-15T14:59:46.330224Z","shell.execute_reply":"2023-07-15T14:59:46.336820Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"6. The quotes from the corpus that generate the highest similarity score with the question input (corresponding to movie or genre) are collected and kept ready for the script generation process","metadata":{}},{"cell_type":"code","source":"story_starters = []\nsearch_results = semantic_search('drama')\nfor _,row in search_results.iterrows():\n    story_starters.append(row.text)\nstory_starters","metadata":{"execution":{"iopub.status.busy":"2023-07-15T15:00:20.451482Z","iopub.execute_input":"2023-07-15T15:00:20.451938Z","iopub.status.idle":"2023-07-15T15:00:20.521832Z","shell.execute_reply.started":"2023-07-15T15:00:20.451905Z","shell.execute_reply":"2023-07-15T15:00:20.520888Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"['The life of every individual is really always a tragedy, but gone through in detail, it has the character of a comedy.',\n 'Comedy aims at representing men as worse, Tragedy as better than in actual life.',\n 'Enjoy life. This is not a dress rehearsal.',\n 'PLOT is CHARACTER revealed by ACTION.',\n 'In a true tragedy, both parties must be right.']"},"metadata":{}}]},{"cell_type":"markdown","source":"7. A text generation pipeline is created from the gpt2-like model fine-tuned and saved earlier on the huggingface model hub.The search results gathered from the previous step are then passed to the pipeline for obtaining the corresponding story scripts","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\nstory_teller = pipeline('text-generation',model='tiny-random-GPT2LMHeadModel-finetuned-corpus',max_length=250)","metadata":{"execution":{"iopub.status.busy":"2023-07-15T15:33:29.061204Z","iopub.execute_input":"2023-07-15T15:33:29.062486Z","iopub.status.idle":"2023-07-15T15:33:31.481424Z","shell.execute_reply.started":"2023-07-15T15:33:29.062437Z","shell.execute_reply":"2023-07-15T15:33:31.478049Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"8. Next gradio interfaces will be created and uploaded to huggingface Spaces to ensure that a user is able to witness a demonstration of the model","metadata":{}},{"cell_type":"code","source":"!pip install gradio\nimport gradio as gr","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_script(search):\n    seed_quote = semantic_search(search)\n    best_quote_result = seed_quote.reset_index().loc[0,'text']\n    return story_teller('\"'+best_quote_result+'\"')[0]['generated_text']\n\ntextbox = gr.Textbox(label='Enter emotion/genre',lines=2,placeholder='Suspense')\ndesc = 'The application generates a story scipt of approximately 250 word length when an emotion or genre is provided as input'\ngr.Interface(fn=create_script,inputs=textbox,outputs='text',\n            title='Story Script Generator',description=desc).launch()","metadata":{"execution":{"iopub.status.busy":"2023-07-15T15:47:02.322272Z","iopub.execute_input":"2023-07-15T15:47:02.323257Z","iopub.status.idle":"2023-07-15T15:47:24.656887Z","shell.execute_reply.started":"2023-07-15T15:47:02.323220Z","shell.execute_reply":"2023-07-15T15:47:24.655595Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Kaggle notebooks require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\nRunning on local URL:  http://127.0.0.1:7863\nRunning on public URL: https://e0bd8530aec96b92c3.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://e0bd8530aec96b92c3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}]}]}